{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04 ‚Äî LSTM Model Architecture\n",
        "## Build the Character-Level Language Model\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Concept Primer\n",
        "\n",
        "### LSTM Architecture Overview\n",
        "\n",
        "Our model has **three layers**:\n",
        "\n",
        "```\n",
        "Input (char IDs) [B, T]\n",
        "        ‚Üì\n",
        "1. EMBEDDING: IDs ‚Üí Dense Vectors\n",
        "   [B, T] ‚Üí [B, T, embedding_dim]\n",
        "        ‚Üì\n",
        "2. LSTM: Process Sequence with Memory\n",
        "   [B, T, embedding_dim] ‚Üí [B, T, hidden_size]\n",
        "   (also updates hidden state h and cell state c)\n",
        "        ‚Üì\n",
        "3. LINEAR: Project to Vocabulary Size\n",
        "   [B, T, hidden_size] ‚Üí [B, T, vocab_size]\n",
        "        ‚Üì\n",
        "Output (logits) [B*T, vocab_size] (reshaped for loss)\n",
        "```\n",
        "\n",
        "### Why Embedding?\n",
        "\n",
        "**Character IDs are categorical** (0, 1, 2, ..., vocab_size-1). They have no inherent order or relationship.\n",
        "\n",
        "**Embeddings** convert sparse IDs to dense vectors that can learn:\n",
        "- Similar characters (vowels vs. consonants)\n",
        "- Positional patterns\n",
        "- Contextual relationships\n",
        "\n",
        "### LSTM Gates Refresher\n",
        "\n",
        "- **Forget gate**: What to remove from cell state\n",
        "- **Input gate**: What new info to store\n",
        "- **Output gate**: What to output from cell state\n",
        "\n",
        "These gates let LSTM remember long-range dependencies (e.g., matching quotes, sentence structure).\n",
        "\n",
        "### Hidden State vs. Cell State\n",
        "\n",
        "- **Cell state (`c`)**: Long-term memory (the \\\"conveyor belt\\\")\n",
        "- **Hidden state (`h`)**: Short-term output (what we pass to next layer)\n",
        "\n",
        "Both have shape: `[num_layers, batch_size, hidden_size]`\n",
        "\n",
        "### What Breaks If We Skip This?\n",
        "\n",
        "- No embedding = model can't learn char relationships\n",
        "- Wrong shapes = crashes during forward pass\n",
        "- No state initialization = unpredictable behavior\n",
        "\n",
        "### Shapes Summary\n",
        "\n",
        "| Component | Input | Output |\n",
        "|-----------|-------|--------|\n",
        "| **Embedding** | `[B, T]` | `[B, T, E]` |\n",
        "| **LSTM** | `[B, T, E]` | `[B, T, H]` (+ states) |\n",
        "| **Linear** | `[B, T, H]` | `[B, T, V]` |\n",
        "| **Reshape** | `[B, T, V]` | `[B*T, V]` |\n",
        "\n",
        "Where:\n",
        "- B = batch_size (36)\n",
        "- T = seq_length (48)\n",
        "- E = embedding_dim (48)\n",
        "- H = hidden_size (96)\n",
        "- V = vocab_size (~50-80)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Objectives\n",
        "\n",
        "By the end of this notebook, you should:\n",
        "\n",
        "- [ ] Define `CharacterLSTM` class inheriting from `nn.Module`\n",
        "- [ ] Implement `__init__` with Embedding, LSTM, and Linear layers\n",
        "- [ ] Implement `forward(x, states)` method that processes a batch\n",
        "- [ ] Implement `init_state(batch_size)` to create initial (h0, c0)\n",
        "- [ ] Instantiate the model and print its architecture\n",
        "- [ ] Test forward pass with a fake batch to verify shapes\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéì Acceptance Criteria\n",
        "\n",
        "**You pass this notebook when:**\n",
        "\n",
        "‚úÖ `print(model)` shows all three layers  \n",
        "‚úÖ Forward pass on fake batch `[36, 48]` returns logits `[36*48, vocab_size]`  \n",
        "‚úÖ Forward pass also returns updated states `(h, c)` with shape `[1, 36, 96]`  \n",
        "‚úÖ You can explain each layer's purpose\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 0: Imports\n",
        "\n",
        "**Import PyTorch modules**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.8.0\n",
            "Using vocab_size: 60\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# We'll need vocab_size from previous notebooks\n",
        "# For now, let's set it (you'll load from previous work in practice)\n",
        "vocab_size = 60  # Approximate; adjust after running notebook 02\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Using vocab_size: {vocab_size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 1: Define Class and `__init__`\n",
        "\n",
        "**Hint:**  \n",
        "Define layers as `self` attributes.\n",
        "\n",
        "**Steps:**\n",
        "1. Class `CharacterLSTM(nn.Module)`\n",
        "2. `__init__(self, vocab_size, embedding_dim=48, hidden_size=96)`\n",
        "3. Call `super().__init__()`\n",
        "4. Create three layers:\n",
        "   - `self.embedding = nn.Embedding(vocab_size, embedding_dim)`\n",
        "   - `self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)`\n",
        "   - `self.fc = nn.Linear(hidden_size, vocab_size)`\n",
        "5. Store `self.hidden_size` for later use\n",
        "\n",
        "**Why `batch_first=True`?**  \n",
        "Makes input/output shape `[B, T, ...]` instead of `[T, B, ...]` ‚Äî easier to work with.\n",
        "\n",
        "**Hyperparameters:**\n",
        "- `embedding_dim=48`: Dense vector size for each character\n",
        "- `hidden_size=96`: LSTM's memory capacity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Define CharacterLSTM with __init__\n",
        "\n",
        "class CharacterLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=48, hidden_size=96):\n",
        "        # TODO: Call super().__init__()\n",
        "        super(CharacterLSTM, self).__init__()\n",
        "        \n",
        "        # TODO: Define layers\n",
        "        # self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
        "        # self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "        \n",
        "        # TODO: Store hidden_size for init_state\n",
        "        # self.hidden_size = hidden_size\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        pass  \n",
        "    \n",
        "    # We'll add forward and init_state next\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 2: Implement `forward` Method\n",
        "\n",
        "**Hint:**  \n",
        "Chain the three layers together.\n",
        "\n",
        "**Steps:**\n",
        "1. `def forward(self, x, states):`\n",
        "   - `x`: input batch `[B, T]` of character IDs\n",
        "   - `states`: tuple `(h, c)` of hidden/cell states\n",
        "2. **Embedding**: `embedded = self.embedding(x)`  ‚Üí `[B, T, E]`\n",
        "3. **LSTM**: `lstm_out, new_states = self.lstm(embedded, states)`  ‚Üí `[B, T, H]`\n",
        "4. **Linear**: `logits = self.fc(lstm_out)`  ‚Üí `[B, T, V]`\n",
        "5. **Reshape**: `logits_flat = logits.view(-1, vocab_size)`  ‚Üí `[B*T, V]`\n",
        "6. **Return**: `(logits_flat, new_states)`\n",
        "\n",
        "**Why reshape?**  \n",
        "`CrossEntropyLoss` expects 2D logits `[N, C]` where N=num_samples, C=num_classes.\n",
        "\n",
        "**Why return states?**  \n",
        "During generation, we'll need to carry states across time steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Add forward method to CharacterLSTM\n",
        "\n",
        "class CharacterLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=48, hidden_size=96):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "        self.hidden_size = hidden_size\n",
        "    \n",
        "    def forward(self, x, states):\n",
        "        # TODO: Pass x through embedding\n",
        "        # embedded = self.embedding(x)  # [B, T] ‚Üí [B, T, E]\n",
        "        embedded = self.embedding(x)  # [B, T] ‚Üí [B, T, E]\n",
        "        \n",
        "        # TODO: Pass through LSTM with states\n",
        "        # lstm_out, new_states = self.lstm(embedded, states)  # [B, T, E] ‚Üí [B, T, H]\n",
        "        lstm_out, new_states = self.lstm(embedded, states)  # [B, T, E] ‚Üí [B, T, H]\n",
        "        \n",
        "        # TODO: Pass through linear layer\n",
        "        # logits = self.fc(lstm_out)  # [B, T, H] ‚Üí [B, T, V]\n",
        "        logits = self.fc(lstm_out)  # [B, T, H] ‚Üí [B, T, V]\n",
        "        \n",
        "        # TODO: Reshape for CrossEntropyLoss\n",
        "        # logits_flat = logits.view(-1, logits.size(-1))  # [B, T, V] ‚Üí [B*T, V]\n",
        "        logits_flat = logits.view(-1, logits.size(-1))  # [B, T, V] ‚Üí [B*T, V]\n",
        "        \n",
        "        # TODO: Return logits and new states\n",
        "        # return logits_flat, new_states\n",
        "        return logits_flat, new_states\n",
        "        \n",
        "        pass\n",
        "    \n",
        "    # We'll add init_state next\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 3: Implement `init_state` Method\n",
        "\n",
        "**Hint:**  \n",
        "Create zero tensors for initial hidden and cell states.\n",
        "\n",
        "**Steps:**\n",
        "1. `def init_state(self, batch_size):`\n",
        "2. Create `h0 = torch.zeros(1, batch_size, self.hidden_size)`\n",
        "   - Shape: `[num_layers, batch_size, hidden_size]`\n",
        "   - We have 1 LSTM layer, so first dim = 1\n",
        "3. Create `c0 = torch.zeros(1, batch_size, self.hidden_size)`\n",
        "4. Return `(h0, c0)`\n",
        "\n",
        "**Why zeros?**  \n",
        "At the start of training/generation, we have no prior context, so we initialize to zeros.\n",
        "\n",
        "**Why shape `[1, B, H]`?**  \n",
        "PyTorch LSTM expects states with shape `[num_layers * num_directions, batch, hidden_size]`.  \n",
        "We have 1 layer, uni-directional ‚Üí first dim = 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Complete CharacterLSTM with init_state\n",
        "\n",
        "class CharacterLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=48, hidden_size=96):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "        self.hidden_size = hidden_size\n",
        "    \n",
        "    def forward(self, x, states):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, new_states = self.lstm(embedded, states)\n",
        "        logits = self.fc(lstm_out)\n",
        "        logits_flat = logits.view(-1, logits.size(-1))\n",
        "        return logits_flat, new_states\n",
        "    \n",
        "    def init_state(self, batch_size):\n",
        "        # TODO: Create hidden state (h0) and cell state (c0)\n",
        "        # h0 = torch.zeros(1, batch_size, self.hidden_size)\n",
        "        # c0 = torch.zeros(1, batch_size, self.hidden_size)\n",
        "        # return (h0, c0)\n",
        "        h0 = torch.zeros(1, batch_size, self.hidden_size)\n",
        "        c0 = torch.zeros(1, batch_size, self.hidden_size)\n",
        "        return (h0, c0)\n",
        "        \n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 4: Instantiate the Model\n",
        "\n",
        "**Steps:**\n",
        "1. Create `model = CharacterLSTM(vocab_size)`\n",
        "2. Print the model architecture with `print(model)`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CharacterLSTM(\n",
            "  (embedding): Embedding(60, 48)\n",
            "  (lstm): LSTM(48, 96, batch_first=True)\n",
            "  (fc): Linear(in_features=96, out_features=60, bias=True)\n",
            ")\n",
            "\n",
            "Total parameters: 64,764\n"
          ]
        }
      ],
      "source": [
        "# TODO: Instantiate the model\n",
        "# model = CharacterLSTM(vocab_size)\n",
        "# print(model)\n",
        "\n",
        "model = CharacterLSTM(vocab_size)  # Replace this line\n",
        "\n",
        "if model:\n",
        "    print(model)\n",
        "    print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 5: Test Forward Pass\n",
        "\n",
        "**Hint:**  \n",
        "Create fake input and states, then call `model(x, states)`.\n",
        "\n",
        "**Steps:**\n",
        "1. Create fake input: `fake_batch = torch.randint(0, vocab_size, (36, 48))`\n",
        "   - Random IDs, shape `[36, 48]`\n",
        "2. Initialize states: `states = model.init_state(36)`\n",
        "3. Forward pass: `logits, new_states = model(fake_batch, states)`\n",
        "4. Print shapes:\n",
        "   - `logits.shape` should be `[36*48, vocab_size]` = `[1728, vocab_size]`\n",
        "   - `new_states[0].shape` (h) should be `[1, 36, 96]`\n",
        "   - `new_states[1].shape` (c) should be `[1, 36, 96]`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shape: torch.Size([36, 48])\n",
            "Logits shape: torch.Size([1728, 60])\n",
            "New hidden state shape: torch.Size([1, 36, 96])\n",
            "New cell state shape: torch.Size([1, 36, 96])\n"
          ]
        }
      ],
      "source": [
        "# TODO: Test forward pass with fake data\n",
        "# fake_batch = torch.randint(0, vocab_size, (36, 48))\n",
        "# states = model.init_state(36)\n",
        "# logits, new_states = model(fake_batch, states)\n",
        "\n",
        "# print(f\"Input shape: {fake_batch.shape}\")\n",
        "# print(f\"Logits shape: {logits.shape}\")\n",
        "# print(f\"New hidden state shape: {new_states[0].shape}\")\n",
        "# print(f\"New cell state shape: {new_states[1].shape}\")\n",
        "\n",
        "# Your code here\n",
        "fake_batch = torch.randint(0, vocab_size, (36, 48))\n",
        "states = model.init_state(36)\n",
        "logits, new_states = model(fake_batch, states)\n",
        "print(f\"Input shape: {fake_batch.shape}\")\n",
        "print(f\"Logits shape: {logits.shape}\")\n",
        "print(f\"New hidden state shape: {new_states[0].shape}\")\n",
        "print(f\"New cell state shape: {new_states[1].shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üí≠ Reflection Prompts\n",
        "\n",
        "**Write your observations:**\n",
        "\n",
        "1. **Three layers**: What is the purpose of each layer (Embedding, LSTM, Linear)?\n",
        "\n",
        "2. **LSTM gates**: In your own words, what do the forget, input, and output gates control?\n",
        "\n",
        "3. **Hidden vs. Cell**: What's the difference between hidden state (h) and cell state (c)?\n",
        "\n",
        "4. **Why reshape**: Why do we reshape from `[B, T, V]` to `[B*T, V]` before returning?\n",
        "\n",
        "5. **Parameter count**: How many parameters does your model have? Where do most come from?\n",
        "\n",
        "6. **batch_first**: What would happen if we set `batch_first=False` in the LSTM?\n",
        "\n",
        "---\n",
        "ddddddddddd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Next Steps\n",
        "\n",
        "Once you've completed all TODOs and verified forward pass shapes:\n",
        "\n",
        "‚û°Ô∏è **Move to Notebook 05**: Train the LSTM model\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Key Takeaways\n",
        "\n",
        "- ‚úÖ `nn.Embedding` converts sparse IDs to dense vectors\n",
        "- ‚úÖ `nn.LSTM` processes sequences with memory (h and c states)\n",
        "- ‚úÖ `nn.Linear` projects to vocabulary size for predictions\n",
        "- ‚úÖ Forward returns both logits and updated states\n",
        "- ‚úÖ States must be initialized to zeros at the start\n",
        "- ‚úÖ Reshaping to `[B*T, V]` prepares for CrossEntropyLoss\n",
        "\n",
        "---\n",
        "\n",
        "*Next up: Training the model with real data and watching loss decrease!*\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Codecademy ML",
      "language": "python",
      "name": "codeacademy"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
