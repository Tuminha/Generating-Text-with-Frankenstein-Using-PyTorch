{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 ‚Äî Sequences, Dataset & DataLoader\n",
        "## Creating Training Data with Sliding Windows\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Concept Primer\n",
        "\n",
        "### What Are Sliding Windows?\n",
        "\n",
        "Language modeling uses **sliding windows** to create (input, target) pairs:\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "Text IDs: [5, 8, 12, 15, 20, 23, 30, 35]\n",
        "seq_length = 4\n",
        "\n",
        "Sample 1:\n",
        "  Features: [5, 8, 12, 15]     ‚Üê input sequence\n",
        "  Labels:   [8, 12, 15, 20]    ‚Üê shifted right by 1\n",
        "\n",
        "Sample 2:\n",
        "  Features: [8, 12, 15, 20]\n",
        "  Labels:   [12, 15, 20, 23]\n",
        "\n",
        "...and so on\n",
        "```\n",
        "\n",
        "**Key Insight**: The labels are the features shifted by one position. This teaches the model: *given these characters, predict the next one*.\n",
        "\n",
        "### Why PyTorch Dataset & DataLoader?\n",
        "\n",
        "**Dataset**: Custom class that:\n",
        "- Stores the data (our ID sequence)\n",
        "- Knows how many samples exist (`__len__`)\n",
        "- Can fetch one sample at a time (`__getitem__`)\n",
        "\n",
        "**DataLoader**: Handles:\n",
        "- Batching (grouping multiple samples)\n",
        "- Shuffling (randomizing order each epoch)\n",
        "- Parallel loading (optional, not needed here)\n",
        "\n",
        "### What Breaks If We Skip This?\n",
        "\n",
        "- No batching = training one sample at a time (slow!)\n",
        "- Manual batching = error-prone, hard to maintain\n",
        "- No Dataset = can't use PyTorch's ecosystem\n",
        "\n",
        "### Shapes\n",
        "- **Single sample features**: `[seq_length]` (e.g., `[48]`)\n",
        "- **Single sample labels**: `[seq_length]` (e.g., `[48]`)\n",
        "- **Batch features**: `[batch_size, seq_length]` (e.g., `[36, 48]`)\n",
        "- **Batch labels**: `[batch_size, seq_length]` (e.g., `[36, 48]`)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Objectives\n",
        "\n",
        "By the end of this notebook, you should:\n",
        "\n",
        "- [ ] Understand sliding window concept for sequence prediction\n",
        "- [ ] Create a custom `TextDataset` class with `__init__`, `__len__`, `__getitem__`\n",
        "- [ ] Implement feature/label shifting (labels = features + 1 position)\n",
        "- [ ] Instantiate the dataset with `seq_length=48`\n",
        "- [ ] Create a `DataLoader` with `batch_size=36`, `shuffle=True`\n",
        "- [ ] Print one batch to verify shapes: `[36, 48]`\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéì Acceptance Criteria\n",
        "\n",
        "**You pass this notebook when:**\n",
        "\n",
        "‚úÖ `TextDataset` class is defined with all three methods  \n",
        "‚úÖ One batch prints with shapes `[36, 48]` for both features and labels  \n",
        "‚úÖ You can explain: \"Why are labels shifted by 1?\"  \n",
        "‚úÖ You understand: \"What does `__len__` return and why `len(ids) - seq_length`?\"\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 0: Setup & Imports\n",
        "\n",
        "**Note:** Load the variables from previous notebooks + import PyTorch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Load data from previous notebooks\n",
        "with open('../datasets/frankenstein.txt', 'r', encoding='utf-8') as f:\n",
        "    frankenstein = f.read()\n",
        "    \n",
        "first_letter_text = frankenstein[1380:8230]\n",
        "tokenized_text = list(first_letter_text)\n",
        "unique_char_tokens = sorted(set(tokenized_text))\n",
        "c2ix = {char: idx for idx, char in enumerate(unique_char_tokens)}\n",
        "ix2c = {idx: char for char, idx in c2ix.items()}\n",
        "vocab_size = len(c2ix)\n",
        "tokenized_id_text = [c2ix[char] for char in tokenized_text]\n",
        "\n",
        "print(f\"Data loaded: {len(tokenized_id_text)} IDs, vocab_size={vocab_size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 1: Define TextDataset Class ‚Äî `__init__`\n",
        "\n",
        "**Hint:**  \n",
        "Store the ID list and sequence length.\n",
        "\n",
        "**Steps:**\n",
        "1. Define class `TextDataset` that inherits from `torch.utils.data.Dataset`\n",
        "2. In `__init__(self, tokenized_ids, seq_length)`:\n",
        "   - Store `self.ids = tokenized_ids`\n",
        "   - Store `self.seq_length = seq_length`\n",
        "\n",
        "**Why?**  \n",
        "The `__init__` method saves the data and hyperparameters we'll need in other methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Define TextDataset class with __init__\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenized_ids, seq_length):\n",
        "        # TODO: Store tokenized_ids and seq_length\n",
        "        # self.ids = ...\n",
        "        # self.seq_length = ...\n",
        "        pass  # Remove this line after implementing\n",
        "    \n",
        "    # We'll add __len__ and __getitem__ next\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 2: Implement `__len__` Method\n",
        "\n",
        "**Hint:**  \n",
        "How many sliding windows can we fit?\n",
        "\n",
        "**Logic:**\n",
        "- We have `len(self.ids)` total IDs\n",
        "- Each window needs `seq_length` IDs for features + 1 more for the last label\n",
        "- Number of samples = `len(self.ids) - seq_length`\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "IDs: [1, 2, 3, 4, 5, 6]  (6 IDs)\n",
        "seq_length = 3\n",
        "\n",
        "Sample 0: features=[1,2,3], labels=[2,3,4]\n",
        "Sample 1: features=[2,3,4], labels=[3,4,5]\n",
        "Sample 2: features=[3,4,5], labels=[4,5,6]\n",
        "\n",
        "Total samples = 6 - 3 = 3\n",
        "```\n",
        "\n",
        "**Return:**  \n",
        "`len(self.ids) - self.seq_length`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Add __len__ to TextDataset\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenized_ids, seq_length):\n",
        "        self.ids = tokenized_ids\n",
        "        self.seq_length = seq_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        # TODO: Return the number of possible sliding windows\n",
        "        # return len(self.ids) - self.seq_length\n",
        "        pass  # Remove this line after implementing\n",
        "    \n",
        "    # We'll add __getitem__ next\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 3: Implement `__getitem__` Method\n",
        "\n",
        "**Hint:**  \n",
        "Slice the ID sequence to get features and labels.\n",
        "\n",
        "**Logic:**\n",
        "- Features: `self.ids[idx : idx + seq_length]`\n",
        "- Labels: `self.ids[idx + 1 : idx + seq_length + 1]` (shifted by 1)\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "self.ids = [10, 20, 30, 40, 50, 60]\n",
        "seq_length = 3\n",
        "idx = 1\n",
        "\n",
        "Features: self.ids[1:4] = [20, 30, 40]\n",
        "Labels:   self.ids[2:5] = [30, 40, 50]  ‚Üê shifted right by 1\n",
        "```\n",
        "\n",
        "**Return:**  \n",
        "- Convert to PyTorch tensors: `torch.tensor(..., dtype=torch.long)`\n",
        "- Return tuple: `(features_tensor, labels_tensor)`\n",
        "\n",
        "**Why `torch.long`?**  \n",
        "IDs are integers, and CrossEntropyLoss expects `long` type.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Complete TextDataset with __getitem__\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenized_ids, seq_length):\n",
        "        self.ids = tokenized_ids\n",
        "        self.seq_length = seq_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.ids) - self.seq_length\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # TODO: Slice features and labels\n",
        "        # features = self.ids[idx : idx + self.seq_length]\n",
        "        # labels = self.ids[idx + 1 : idx + self.seq_length + 1]\n",
        "        \n",
        "        # TODO: Convert to tensors\n",
        "        # features_tensor = torch.tensor(features, dtype=torch.long)\n",
        "        # labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "        \n",
        "        # TODO: Return the tuple\n",
        "        # return features_tensor, labels_tensor\n",
        "        pass  # Remove this line after implementing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 4: Instantiate the Dataset\n",
        "\n",
        "**Hint:**  \n",
        "Create an instance with your tokenized IDs and `seq_length=48`.\n",
        "\n",
        "**Steps:**\n",
        "1. `dataset = TextDataset(tokenized_id_text, seq_length=48)`\n",
        "2. Print `len(dataset)` to see how many samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create dataset instance\n",
        "# dataset = TextDataset(tokenized_id_text, seq_length=48)\n",
        "\n",
        "dataset = None  # Replace this line\n",
        "\n",
        "# Verify\n",
        "if dataset:\n",
        "    print(f\"Dataset created with {len(dataset)} samples\")\n",
        "    \n",
        "    # Test one sample\n",
        "    features, labels = dataset[0]\n",
        "    print(f\"\\nSample 0:\")\n",
        "    print(f\"  Features shape: {features.shape}\")\n",
        "    print(f\"  Labels shape: {labels.shape}\")\n",
        "    print(f\"  First 10 feature IDs: {features[:10].tolist()}\")\n",
        "    print(f\"  First 10 label IDs: {labels[:10].tolist()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 5: Create DataLoader\n",
        "\n",
        "**Hint:**  \n",
        "Use `torch.utils.data.DataLoader` to batch and shuffle.\n",
        "\n",
        "**Steps:**\n",
        "1. Create `DataLoader(dataset, batch_size=36, shuffle=True)`\n",
        "2. Why `shuffle=True`? Randomizes training order each epoch for better generalization\n",
        "\n",
        "**Batch size 36:**  \n",
        "Arbitrary choice. Smaller batches = more updates but noisier gradients. Larger = more stable but fewer updates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create DataLoader\n",
        "# dataloader = DataLoader(dataset, batch_size=36, shuffle=True)\n",
        "\n",
        "dataloader = None  # Replace this line\n",
        "\n",
        "# Verify\n",
        "if dataloader:\n",
        "    print(f\"DataLoader created with batch_size=36, shuffle=True\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 6: Test the DataLoader\n",
        "\n",
        "**Hint:**  \n",
        "Iterate once to get a batch and print shapes.\n",
        "\n",
        "**Steps:**\n",
        "1. Loop: `for batch_features, batch_labels in dataloader:`\n",
        "2. Print shapes (should be `[36, 48]` for both)\n",
        "3. `break` after first batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Get one batch and print shapes\n",
        "# for batch_features, batch_labels in dataloader:\n",
        "#     print(f\"Batch features shape: {batch_features.shape}\")\n",
        "#     print(f\"Batch labels shape: {batch_labels.shape}\")\n",
        "#     print(f\"\\nFirst sample in batch:\")\n",
        "#     print(f\"  Features: {batch_features[0][:20]}\")  # First 20 IDs\n",
        "#     print(f\"  Labels:   {batch_labels[0][:20]}\")\n",
        "#     break\n",
        "\n",
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üí≠ Reflection Prompts\n",
        "\n",
        "**Write your observations:**\n",
        "\n",
        "1. **Shifting by 1**: Why are labels exactly the same as features but shifted right by one position?\n",
        "\n",
        "2. **Dataset length**: If we have 6,850 IDs and `seq_length=48`, how many samples do we get? Why?\n",
        "\n",
        "3. **Batch shape**: What does `[36, 48]` mean? (36 = ?, 48 = ?)\n",
        "\n",
        "4. **Shuffling**: What would happen if we set `shuffle=False`? Would the model still learn?\n",
        "\n",
        "5. **Why tensors**: Why do we convert to `torch.tensor` instead of keeping as Python lists?\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Next Steps\n",
        "\n",
        "Once you've completed all TODOs and printed batch shapes:\n",
        "\n",
        "‚û°Ô∏è **Move to Notebook 04**: Define the LSTM model architecture\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Key Takeaways\n",
        "\n",
        "- ‚úÖ Sliding windows create (input, target) pairs for training\n",
        "- ‚úÖ Labels = Features shifted by 1 position\n",
        "- ‚úÖ `Dataset` knows how to fetch one sample\n",
        "- ‚úÖ `DataLoader` handles batching and shuffling\n",
        "- ‚úÖ Batch shape `[B, T]` where B=batch_size, T=seq_length\n",
        "- ‚úÖ All data must be PyTorch tensors for the model\n",
        "\n",
        "---\n",
        "\n",
        "*Next up: Building the LSTM model that will process these batches!*\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
