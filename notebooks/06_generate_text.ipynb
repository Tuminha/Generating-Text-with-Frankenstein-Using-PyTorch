{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 06 ‚Äî Generate Text\n",
        "## Sampling with the Trained LSTM\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Concept Primer\n",
        "\n",
        "### How Text Generation Works\n",
        "\n",
        "Generation is an **autoregressive loop**:\n",
        "\n",
        "```\n",
        "1. Start with a prompt: \"You will rejoice to hear\"\n",
        "2. Feed prompt through model ‚Üí get logits for next char\n",
        "3. Pick next char (greedy: argmax of logits)\n",
        "4. Append char to sequence\n",
        "5. Feed updated sequence ‚Üí get logits for next char\n",
        "6. Repeat until we have 500 characters\n",
        "```\n",
        "\n",
        "### Generation vs. Training\n",
        "\n",
        "| Aspect | Training | Generation |\n",
        "|--------|----------|------------|\n",
        "| **Goal** | Learn from data | Produce new text |\n",
        "| **Mode** | `model.train()` | `model.eval()` |\n",
        "| **Gradients** | Needed | `torch.no_grad()` |\n",
        "| **Input** | Real text batches | Generated chars |\n",
        "| **Output** | Loss | New characters |\n",
        "\n",
        "### Greedy Sampling\n",
        "\n",
        "**Argmax**: Always pick the most likely character.\n",
        "\n",
        "```python\n",
        "logits = model(...)  # [1, vocab_size]\n",
        "next_id = torch.argmax(logits).item()\n",
        "```\n",
        "\n",
        "**Pros**: Simple, deterministic  \n",
        "**Cons**: Repetitive, no creativity\n",
        "\n",
        "**Alternative**: Temperature sampling (adds randomness) ‚Äî left as an extension.\n",
        "\n",
        "### States in Generation\n",
        "\n",
        "Unlike training (batch-level states), generation:\n",
        "- Uses **single batch size = 1**\n",
        "- **Carries states** across time steps (maintains context)\n",
        "- Feeds one character at a time\n",
        "\n",
        "### What Breaks If We Skip This?\n",
        "\n",
        "- No `eval()` = dropout/batchnorm behave incorrectly\n",
        "- Gradients tracked = slow + memory leak\n",
        "- Wrong prompt tokenization = crashes or gibberish\n",
        "\n",
        "### Shapes During Generation\n",
        "\n",
        "| Step | Shape |\n",
        "|------|-------|\n",
        "| **Prompt IDs** | `[1, prompt_length]` |\n",
        "| **Single char input** | `[1, 1]` |\n",
        "| **Logits** | `[1, vocab_size]` |\n",
        "| **States (h, c)** | `[1, 1, 96]` each |\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Objectives\n",
        "\n",
        "By the end of this notebook, you should:\n",
        "\n",
        "- [ ] Load the trained model weights\n",
        "- [ ] Set the model to `eval()` mode\n",
        "- [ ] Define a starting prompt: `\"You will rejoice to hear\"`\n",
        "- [ ] Tokenize the prompt to IDs\n",
        "- [ ] Initialize states for batch size = 1\n",
        "- [ ] Implement generation loop to produce 500 characters\n",
        "- [ ] Decode IDs back to text and print\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéì Acceptance Criteria\n",
        "\n",
        "**You pass this notebook when:**\n",
        "\n",
        "‚úÖ 500 characters of generated text print without errors  \n",
        "‚úÖ Generated text looks vaguely Frankenstein-ish (Gothic, archaic style)  \n",
        "‚úÖ You can explain the difference between greedy and temperature sampling\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 0: Setup ‚Äî Load Data, Model, Weights\n",
        "\n",
        "**Load vocab mappings, define model, load trained weights**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary loaded: 60 unique characters\n",
            "Model loaded and set to eval() mode\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# === Load vocabulary mappings (from notebook 02) ===\n",
        "with open('../datasets/frankenstein.txt', 'r', encoding='utf-8') as f:\n",
        "    frankenstein = f.read()\n",
        "    \n",
        "first_letter_text = frankenstein[1380:8230]\n",
        "tokenized_text = list(first_letter_text)\n",
        "unique_char_tokens = sorted(set(tokenized_text))\n",
        "c2ix = {char: idx for idx, char in enumerate(unique_char_tokens)}\n",
        "ix2c = {idx: char for char, idx in c2ix.items()}\n",
        "vocab_size = len(c2ix)\n",
        "\n",
        "print(f\"Vocabulary loaded: {vocab_size} unique characters\")\n",
        "\n",
        "# === Define Model (same as before) ===\n",
        "class CharacterLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=48, hidden_size=96):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "        self.hidden_size = hidden_size\n",
        "    \n",
        "    def forward(self, x, states):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, new_states = self.lstm(embedded, states)\n",
        "        logits = self.fc(lstm_out)\n",
        "        logits_flat = logits.view(-1, logits.size(-1))\n",
        "        return logits_flat, new_states\n",
        "    \n",
        "    def init_state(self, batch_size):\n",
        "        h0 = torch.zeros(1, batch_size, self.hidden_size)\n",
        "        c0 = torch.zeros(1, batch_size, self.hidden_size)\n",
        "        return (h0, c0)\n",
        "\n",
        "# === Instantiate and load trained weights ===\n",
        "model = CharacterLSTM(vocab_size)\n",
        "model.load_state_dict(torch.load('../src/models/trained_lstm_model.pth'))\n",
        "model.eval()  # Set to evaluation mode\n",
        "\n",
        "print(\"Model loaded and set to eval() mode\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 1: Define Prompt and Tokenize\n",
        "\n",
        "**Hint:**  \n",
        "Convert prompt string ‚Üí list of char IDs.\n",
        "\n",
        "**Steps:**\n",
        "1. Define `starting_prompt = \"You will rejoice to hear\"`\n",
        "2. Convert to list of IDs: `[c2ix[char] for char in starting_prompt]`\n",
        "3. Convert to tensor: `torch.tensor(..., dtype=torch.long).unsqueeze(0)`\n",
        "   - `unsqueeze(0)` adds batch dimension: `[prompt_length]` ‚Üí `[1, prompt_length]`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: 'You will rejoice to hear that no disaster has accompanied the commencement of an enterprise'\n",
            "Prompt tensor shape: torch.Size([1, 91])\n"
          ]
        }
      ],
      "source": [
        "# TODO: Define and tokenize the starting prompt\n",
        "# starting_prompt = \"You will rejoice to hear\"\n",
        "# prompt_ids = [c2ix[char] for char in starting_prompt]\n",
        "# prompt_tensor = torch.tensor(prompt_ids, dtype=torch.long).unsqueeze(0)  # [1, prompt_length]\n",
        "\n",
        "starting_prompt = \"You will rejoice to hear that no disaster has accompanied the commencement of an enterprise\"\n",
        "prompt_ids = [c2ix[char] for char in starting_prompt]\n",
        "prompt_tensor = torch.tensor(prompt_ids, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "if starting_prompt and prompt_tensor is not None:\n",
        "    print(f\"Prompt: '{starting_prompt}'\")\n",
        "    print(f\"Prompt tensor shape: {prompt_tensor.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 2: Warm Up States with Prompt\n",
        "\n",
        "**Hint:**  \n",
        "Feed the prompt through the model to initialize states.\n",
        "\n",
        "**Steps:**\n",
        "1. Initialize states: `states = model.init_state(1)`\n",
        "2. With `torch.no_grad():`\n",
        "3. Feed prompt: `logits, states = model(prompt_tensor, states)`\n",
        "4. Get last logits: `last_logits = logits[-1:]`  (shape `[1, vocab_size]`)\n",
        "\n",
        "**Why this step?**  \n",
        "The prompt \"primes\" the model with context. The resulting states carry memory of \"You will rejoice to hear\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "States warmed up. Last logits shape: torch.Size([1, 60])\n"
          ]
        }
      ],
      "source": [
        "# TODO: Feed prompt to warm up states\n",
        "# states = model.init_state(1)\n",
        "# \n",
        "# with torch.no_grad():\n",
        "#     logits, states = model(prompt_tensor, states)\n",
        "#     last_logits = logits[-1:]  # Last time step logits\n",
        "\n",
        "states = model.init_state(1)  # Replace\n",
        "with torch.no_grad():\n",
        "    logits, states = model(prompt_tensor, states)\n",
        "    last_logits = logits[-1:]\n",
        "if states and last_logits is not None:\n",
        "    print(f\"States warmed up. Last logits shape: {last_logits.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 3: Generation Loop\n",
        "\n",
        "**Hint:**  \n",
        "Loop 500 times, generating one character per iteration.\n",
        "\n",
        "**Structure:**\n",
        "```python\n",
        "generated_ids = []\n",
        "num_generated_chars = 500\n",
        "\n",
        "with torch.no_grad():\n",
        "    for _ in range(num_generated_chars):\n",
        "        # 1. Argmax to get next char ID\n",
        "        next_id = torch.argmax(last_logits).item()\n",
        "        generated_ids.append(next_id)\n",
        "        \n",
        "        # 2. Prepare next input: shape [1, 1]\n",
        "        next_input = torch.tensor([[next_id]], dtype=torch.long)\n",
        "        \n",
        "        # 3. Forward pass\n",
        "        logits, states = model(next_input, states)\n",
        "        last_logits = logits[-1:]\n",
        "```\n",
        "\n",
        "**Key details:**\n",
        "- `torch.argmax(last_logits)` picks most likely char\n",
        "- `.item()` converts tensor to Python int\n",
        "- `[[next_id]]` creates shape `[1, 1]`\n",
        "- States are carried across iterations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 500 character IDs\n"
          ]
        }
      ],
      "source": [
        "# TODO: Generation loop\n",
        "# generated_ids = []\n",
        "# num_generated_chars = 500\n",
        "# \n",
        "# with torch.no_grad():\n",
        "#     for _ in range(num_generated_chars):\n",
        "#         # Get next char ID (greedy sampling)\n",
        "#         next_id = torch.argmax(last_logits).item()\n",
        "#         generated_ids.append(next_id)\n",
        "#         \n",
        "#         # Prepare next input [1, 1]\n",
        "#         next_input = torch.tensor([[next_id]], dtype=torch.long)\n",
        "#         \n",
        "#         # Forward pass\n",
        "#         logits, states = model(next_input, states)\n",
        "#         last_logits = logits[-1:]\n",
        "\n",
        "generated_ids = []  # Replace with your loop\n",
        "num_generated_chars = 500\n",
        "\n",
        "with torch.no_grad():\n",
        "    for _ in range(num_generated_chars):\n",
        "        next_id = torch.argmax(last_logits).item()\n",
        "        generated_ids.append(next_id)\n",
        "        \n",
        "        next_input = torch.tensor([[next_id]], dtype=torch.long)\n",
        "        \n",
        "        logits, states = model(next_input, states)\n",
        "        last_logits = logits[-1:]\n",
        "        \n",
        "\n",
        "if generated_ids:\n",
        "    print(f\"Generated {len(generated_ids)} character IDs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 4: Decode and Print Generated Text\n",
        "\n",
        "**Hint:**  \n",
        "Convert IDs back to characters using `ix2c`.\n",
        "\n",
        "**Steps:**\n",
        "1. Decode: `generated_text = ''.join([ix2c[id] for id in generated_ids])`\n",
        "2. Combine with prompt: `full_text = starting_prompt + generated_text`\n",
        "3. Print the result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "GENERATED TEXT (Prompt + 500 chars):\n",
            "================================================================================\n",
            "You will rejoice to hear that no disaster has accompanied the commencement of an enterprise which have\n",
            "been made in the prospect of arriving at the pole\n",
            "to those countries, to reach welfare you and I may meet. If I succeed, my sister, I will put\n",
            "some trust in preceding navigators‚Äîthere snow and favourable period for one time I try undoubtedly are in the post-road between walking the\n",
            "deck and remaining seated my sister, I will put\n",
            "some trust in preceding navigators‚Äîthere snow and favourable period for one time I try undoubtedly are in the post-road between walking the\n",
            "deck and remainin\n"
          ]
        }
      ],
      "source": [
        "# TODO: Decode generated IDs to text\n",
        "# generated_text = ''.join([ix2c[id] for id in generated_ids])\n",
        "# full_text = starting_prompt + generated_text\n",
        "\n",
        "# print(\"=\"*80)\n",
        "# print(\"GENERATED TEXT (Prompt + 500 chars):\")\n",
        "# print(\"=\"*80)\n",
        "# print(full_text)\n",
        "# print(\"=\"*80)\n",
        "\n",
        "# Your code here\n",
        "generated_text = ''.join([ix2c[id] for id in generated_ids])\n",
        "full_text = starting_prompt + generated_text\n",
        "print(\"=\"*80)\n",
        "print(\"GENERATED TEXT (Prompt + 500 chars):\")\n",
        "print(\"=\"*80)\n",
        "print(full_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üí≠ Reflection Prompts\n",
        "\n",
        "**Write your observations:**\n",
        "\n",
        "1. **Generated style**: Does the generated text resemble Mary Shelley's style? (sentence structure, word choice, punctuation)\n",
        "\n",
        "2. **Coherence**: Is the text coherent over short spans? Long spans?\n",
        "\n",
        "3. **Repetition**: Do you see any repeated phrases or loops?\n",
        "\n",
        "4. **Greedy vs. Sampling**: What would change if we used temperature sampling instead of argmax?\n",
        "\n",
        "5. **Prompt influence**: How much does the starting prompt affect the generated text?\n",
        "\n",
        "6. **Improvements**: What would make the generation better? (More data? Longer training? Larger model?)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Extensions to Try\n",
        "\n",
        "**Want to explore further?**\n",
        "\n",
        "1. **Temperature Sampling**:\n",
        "   ```python\n",
        "   # Instead of argmax:\n",
        "   probs = torch.softmax(last_logits / temperature, dim=-1)\n",
        "   next_id = torch.multinomial(probs, num_samples=1).item()\n",
        "   ```\n",
        "   - `temperature < 1`: More confident (sharper)\n",
        "   - `temperature > 1`: More random (flatter)\n",
        "\n",
        "2. **Longer Generation**: Try 1000 or 2000 characters\n",
        "\n",
        "3. **Different Prompts**: \"I beheld the wretch\", \"It was a dreary night\"\n",
        "\n",
        "4. **Train on Full Novel**: Remove the slice and train on entire *Frankenstein*\n",
        "\n",
        "5. **Beam Search**: Keep top-k candidates at each step\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary loaded: 93 unique characters (built from the whole novel)\n",
            "Dataset: 438762 samples\n",
            "DataLoader: 12188 batches per epoch\n",
            "Epoch 1/10, Loss: 1.5117\n",
            "Epoch 2/10, Loss: 1.5146\n",
            "Epoch 3/10, Loss: 1.6102\n",
            "Epoch 4/10, Loss: 1.6288\n",
            "Epoch 5/10, Loss: 1.6370\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     81\u001b[39m labels_flat = batch_labels.view(-\u001b[32m1\u001b[39m)\n\u001b[32m     82\u001b[39m loss = criterion(logits, labels_flat)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m optimizer.step()\n\u001b[32m     85\u001b[39m epoch_loss += loss.item()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "import os\n",
        "\n",
        "# === Load full text and build vocab from WHOLE NOVEL ===\n",
        "with open('../datasets/frankenstein.txt', 'r', encoding='utf-8') as f:\n",
        "    full_text = f.read()\n",
        "\n",
        "tokenized_text = list(full_text)  # Use the entire novel, not just Letter 1!\n",
        "unique_char_tokens = sorted(set(tokenized_text))\n",
        "c2ix = {char: idx for idx, char in enumerate(unique_char_tokens)}\n",
        "ix2c = {idx: char for char, idx in c2ix.items()}\n",
        "vocab_size = len(c2ix)\n",
        "\n",
        "tokenized_id_text = [c2ix[char] for char in tokenized_text]\n",
        "\n",
        "print(f\"Vocabulary loaded: {vocab_size} unique characters (built from the whole novel)\")\n",
        "\n",
        "# === Define Dataset ===\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenized_ids, seq_length):\n",
        "        self.ids = tokenized_ids\n",
        "        self.seq_length = seq_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.ids) - self.seq_length\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        features = self.ids[idx : idx + self.seq_length]\n",
        "        labels = self.ids[idx + 1 : idx + self.seq_length + 1]\n",
        "        return (\n",
        "            torch.tensor(features, dtype=torch.long),\n",
        "            torch.tensor(labels, dtype=torch.long)\n",
        "        )\n",
        "\n",
        "# === Define Model ===\n",
        "class CharacterLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=48, hidden_size=96):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "        self.hidden_size = hidden_size\n",
        "    \n",
        "    def forward(self, x, states):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, new_states = self.lstm(embedded, states)\n",
        "        logits = self.fc(lstm_out)\n",
        "        logits_flat = logits.view(-1, logits.size(-1))\n",
        "        return logits_flat, new_states\n",
        "    \n",
        "    def init_state(self, batch_size):\n",
        "        h0 = torch.zeros(1, batch_size, self.hidden_size)\n",
        "        c0 = torch.zeros(1, batch_size, self.hidden_size)\n",
        "        return (h0, c0)\n",
        "\n",
        "# === Create Dataset & DataLoader ===\n",
        "dataset = TextDataset(tokenized_id_text, seq_length=48)\n",
        "dataloader = DataLoader(dataset, batch_size=36, shuffle=True)\n",
        "\n",
        "print(f\"Dataset: {len(dataset)} samples\")\n",
        "print(f\"DataLoader: {len(dataloader)} batches per epoch\")\n",
        "\n",
        "# === Instantiate model, loss, optimizer ===\n",
        "char_model = CharacterLSTM(vocab_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(char_model.parameters(), lr=0.015)\n",
        "num_epochs = 10\n",
        "\n",
        "# === Training Loop ===\n",
        "for epoch in range(num_epochs):\n",
        "    char_model.train()\n",
        "    epoch_loss = 0\n",
        "    for batch_features, batch_labels in dataloader:\n",
        "        batch_size = batch_features.size(0)\n",
        "        optimizer.zero_grad()\n",
        "        states = char_model.init_state(batch_size)\n",
        "        logits, new_states = char_model(batch_features, states)\n",
        "        labels_flat = batch_labels.view(-1)\n",
        "        loss = criterion(logits, labels_flat)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    avg_loss = epoch_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "# === Save the trained model ===\n",
        "save_path = '../src/models/new_trained_lstm_model.pth'\n",
        "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "torch.save(char_model.state_dict(), save_path)\n",
        "print(f\"Model saved to {save_path}\")\n",
        "\n",
        "# === Generation with temperature sampling ===\n",
        "char_model.eval()\n",
        "\n",
        "starting_prompt = \"You will rejoice to hear that no disaster has accompanied the commencement of an enterprise\"\n",
        "prompt_ids = [c2ix[char] for char in starting_prompt]\n",
        "prompt_tensor = torch.tensor(prompt_ids, dtype=torch.long).unsqueeze(0)  # shape: (1, prompt_len)\n",
        "\n",
        "if starting_prompt and prompt_tensor is not None:\n",
        "    print(f\"Prompt: '{starting_prompt}'\")\n",
        "    print(f\"Prompt tensor shape: {prompt_tensor.shape}\")\n",
        "\n",
        "temperature = 0.8  # RECOMMENDED: 0.8 for creativity while retaining coherence\n",
        "num_generated_chars = 2000  # Generate 2,000 characters for more realism\n",
        "\n",
        "# Warm up states by running prompt through the model\n",
        "states = char_model.init_state(1)  # batch_size=1 for generation\n",
        "with torch.no_grad():\n",
        "    logits, states = char_model(prompt_tensor, states)\n",
        "    last_logits = logits[-1:]\n",
        "\n",
        "if states is not None and last_logits is not None:\n",
        "    print(f\"States warmed up. Last logits shape: {last_logits.shape}\")\n",
        "\n",
        "generated_ids = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for _ in range(num_generated_chars):\n",
        "        # Temperature sampling instead of argmax!\n",
        "        probs = torch.softmax(last_logits / temperature, dim=-1)\n",
        "        next_id = torch.multinomial(probs, num_samples=1).item()\n",
        "        generated_ids.append(next_id)\n",
        "\n",
        "        # Next input needs to be shape (1, 1)\n",
        "        next_input = torch.tensor([[next_id]], dtype=torch.long)\n",
        "        logits, states = char_model(next_input, states)\n",
        "        last_logits = logits[-1:]\n",
        "\n",
        "if generated_ids:\n",
        "    print(f\"Generated {len(generated_ids)} character IDs\")\n",
        "\n",
        "generated_text = ''.join([ix2c[id] for id in generated_ids])\n",
        "full_generated = starting_prompt + generated_text\n",
        "print(\"=\"*80)\n",
        "print(\"GENERATED TEXT (Prompt + 2000 chars):\")\n",
        "print(\"=\"*80)\n",
        "print(full_generated)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ‚ö†Ô∏è TRAINING ISSUE FIXES NEEDED\n",
        "\n",
        "## Problems Detected:\n",
        "1. **Learning rate too high**: 0.015 is too aggressive for full novel (438K samples)\n",
        "2. **Vocab mismatch**: Model trained on vocab_size=60, but full novel has vocab_size=93\n",
        "3. **Loss increasing**: Model diverging instead of converging\n",
        "\n",
        "## Solutions Below ‚Üì\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary: 93 unique characters\n",
            "Text length: 438810 characters\n",
            "Dataset: 438762 samples\n",
            "Batches per epoch: 12188\n",
            "Model parameters: 69,549\n",
            "\n",
            "Starting training...\n"
          ]
        }
      ],
      "source": [
        "# FIXED TRAINING CELL - Use this instead!\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "import os\n",
        "\n",
        "# === Load FULL text and build vocab ===\n",
        "with open('../datasets/frankenstein.txt', 'r', encoding='utf-8') as f:\n",
        "    full_text = f.read()\n",
        "\n",
        "tokenized_text = list(full_text)\n",
        "unique_char_tokens = sorted(set(tokenized_text))\n",
        "c2ix = {char: idx for idx, char in enumerate(unique_char_tokens)}\n",
        "ix2c = {idx: char for char, idx in c2ix.items()}\n",
        "vocab_size = len(c2ix)\n",
        "\n",
        "tokenized_id_text = [c2ix[char] for char in tokenized_text]\n",
        "\n",
        "print(f\"Vocabulary: {vocab_size} unique characters\")\n",
        "print(f\"Text length: {len(tokenized_text)} characters\")\n",
        "\n",
        "# === Dataset ===\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenized_ids, seq_length):\n",
        "        self.ids = tokenized_ids\n",
        "        self.seq_length = seq_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.ids) - self.seq_length\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        features = self.ids[idx : idx + self.seq_length]\n",
        "        labels = self.ids[idx + 1 : idx + self.seq_length + 1]\n",
        "        return (\n",
        "            torch.tensor(features, dtype=torch.long),\n",
        "            torch.tensor(labels, dtype=torch.long)\n",
        "        )\n",
        "\n",
        "# === Model ===\n",
        "class CharacterLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=48, hidden_size=96):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "        self.hidden_size = hidden_size\n",
        "    \n",
        "    def forward(self, x, states):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, new_states = self.lstm(embedded, states)\n",
        "        logits = self.fc(lstm_out)\n",
        "        logits_flat = logits.view(-1, logits.size(-1))\n",
        "        return logits_flat, new_states\n",
        "    \n",
        "    def init_state(self, batch_size):\n",
        "        h0 = torch.zeros(1, batch_size, self.hidden_size)\n",
        "        c0 = torch.zeros(1, batch_size, self.hidden_size)\n",
        "        return (h0, c0)\n",
        "\n",
        "# === Dataset & DataLoader ===\n",
        "dataset = TextDataset(tokenized_id_text, seq_length=48)\n",
        "dataloader = DataLoader(dataset, batch_size=36, shuffle=True)\n",
        "\n",
        "print(f\"Dataset: {len(dataset)} samples\")\n",
        "print(f\"Batches per epoch: {len(dataloader)}\")\n",
        "\n",
        "# === Model, Loss, Optimizer ===\n",
        "char_model = CharacterLSTM(vocab_size)\n",
        "print(f\"Model parameters: {sum(p.numel() for p in char_model.parameters()):,}\")\n",
        "\n",
        "# CRITICAL FIX: Lower learning rate for larger dataset\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(char_model.parameters(), lr=0.003)  # ‚Üê Reduced from 0.015 to 0.003!\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "# === Training ===\n",
        "print(\"\\nStarting training...\")\n",
        "for epoch in range(num_epochs):\n",
        "    char_model.train()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for batch_features, batch_labels in dataloader:\n",
        "        batch_size = batch_features.size(0)\n",
        "        optimizer.zero_grad()\n",
        "        states = char_model.init_state(batch_size)\n",
        "        logits, new_states = char_model(batch_features, states)\n",
        "        labels_flat = batch_labels.view(-1)\n",
        "        loss = criterion(logits, labels_flat)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    \n",
        "    avg_loss = epoch_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "# Save\n",
        "save_path = '../src/models/trained_lstm_full_novel.pth'\n",
        "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "torch.save(char_model.state_dict(), save_path)\n",
        "print(f\"Model saved to {save_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Accuracy Evaluation\n",
        "\n",
        "**Test the model on known text continuations to measure accuracy**\n",
        "\n",
        "This measures **character-level accuracy** - how many characters the model predicts correctly compared to the actual text continuation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test prompts and their expected continuations from Frankenstein\n",
        "test_prompts = [\n",
        "    \"I am already far north of London, and as I walk in the streets of Petersburgh, I feel a cold northern breeze play upon my cheeks, which braces my nerves and fills\",\n",
        "    \"These reflections have dispelled the agitation with which I began my letter, and I feel my heart glow with an enthusiasm which elevates me to heaven\",\n",
        "    \"These visions faded when I perused, for the first time, those poets whose effusions entranced my soul\"\n",
        "]\n",
        "\n",
        "expected_continuations = [\n",
        "    \" me with delight. Do you understand this feeling? This breeze, which has travelled from the regions towards which I am advancing, gives me a foretaste of those icy climes.\",\n",
        "    \" for nothing contributes so much to tranquillise the mind as a steady purpose‚Äîa point on which the soul may fix its intellectual eye.\",\n",
        "    \" and lifted it to heaven. I also became a poet and for one year lived in a paradise of my own creation;\"\n",
        "]\n",
        "\n",
        "print(f\"Loaded {len(test_prompts)} test prompts\")\n",
        "print(f\"Test 1 length: {len(expected_continuations[0])} chars\")\n",
        "print(f\"Test 2 length: {len(expected_continuations[1])} chars\")\n",
        "print(f\"Test 3 length: {len(expected_continuations[2])} chars\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate accuracy for each prompt\n",
        "def evaluate_accuracy(model, prompt, expected_text, temperature=0.8):\n",
        "    \"\"\"\n",
        "    Generate text from prompt and calculate character-level accuracy.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained LSTM model\n",
        "        prompt: Starting text\n",
        "        expected_text: Ground truth continuation\n",
        "        temperature: Sampling temperature\n",
        "    \n",
        "    Returns:\n",
        "        accuracy: Percentage of correct characters\n",
        "        generated_text: Model's generation\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Tokenize prompt\n",
        "    prompt_ids = [c2ix[char] for char in prompt]\n",
        "    prompt_tensor = torch.tensor(prompt_ids, dtype=torch.long).unsqueeze(0)\n",
        "    \n",
        "    # Initialize states and warm up with prompt\n",
        "    states = model.init_state(1)\n",
        "    with torch.no_grad():\n",
        "        logits, states = model(prompt_tensor, states)\n",
        "        last_logits = logits[-1:]\n",
        "    \n",
        "    # Generate expected length\n",
        "    generated_ids = []\n",
        "    with torch.no_grad():\n",
        "        for char in expected_text:\n",
        "            # Temperature sampling\n",
        "            probs = torch.softmax(last_logits / temperature, dim=-1)\n",
        "            next_id = torch.multinomial(probs, num_samples=1).item()\n",
        "            generated_ids.append(next_id)\n",
        "            \n",
        "            next_input = torch.tensor([[next_id]], dtype=torch.long)\n",
        "            logits, states = model(next_input, states)\n",
        "            last_logits = logits[-1:]\n",
        "    \n",
        "    # Decode generated text\n",
        "    generated_text = ''.join([ix2c[id] for id in generated_ids])\n",
        "    \n",
        "    # Calculate accuracy\n",
        "    correct = sum(1 for g, e in zip(generated_text, expected_text) if g == e)\n",
        "    accuracy = (correct / len(expected_text)) * 100 if len(expected_text) > 0 else 0\n",
        "    \n",
        "    return accuracy, generated_text\n",
        "\n",
        "print(\"Accuracy evaluation function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation on all test prompts\n",
        "results = []\n",
        "\n",
        "for i, (prompt, expected) in enumerate(zip(test_prompts, expected_continuations), 1):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"TEST {i}: Evaluating prompt...\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    accuracy, generated = evaluate_accuracy(model, prompt, expected, temperature=0.8)\n",
        "    results.append({\n",
        "        'prompt_num': i,\n",
        "        'accuracy': accuracy,\n",
        "        'prompt': prompt[:50] + \"...\",\n",
        "        'expected_length': len(expected),\n",
        "        'generated_length': len(generated)\n",
        "    })\n",
        "    \n",
        "    print(f\"\\nPrompt: {prompt[:80]}...\")\n",
        "    print(f\"\\nExpected ({len(expected)} chars):\")\n",
        "    print(f\"  {expected[:100]}...\")\n",
        "    print(f\"\\nGenerated ({len(generated)} chars):\")\n",
        "    print(f\"  {generated[:100]}...\")\n",
        "    print(f\"\\n‚úÖ Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"SUMMARY\")\n",
        "print(f\"{'='*80}\")\n",
        "for r in results:\n",
        "    print(f\"Test {r['prompt_num']}: {r['accuracy']:.2f}% accuracy\")\n",
        "\n",
        "avg_accuracy = sum(r['accuracy'] for r in results) / len(results)\n",
        "print(f\"\\nüéØ Average Accuracy: {avg_accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìå Key Takeaways\n",
        "\n",
        "- ‚úÖ Generation is autoregressive: each char depends on previous chars\n",
        "- ‚úÖ `model.eval()` and `torch.no_grad()` are essential for inference\n",
        "- ‚úÖ Greedy sampling (argmax) is simple but can be repetitive\n",
        "- ‚úÖ States are carried across generation steps to maintain context\n",
        "- ‚úÖ The prompt \"primes\" the model with initial context\n",
        "- ‚úÖ Decoding: IDs ‚Üí characters using `ix2c`\n",
        "\n",
        "---\n",
        "\n",
        "## üéâ Congratulations!\n",
        "\n",
        "You've completed the full pipeline:\n",
        "1. ‚úÖ Loaded and sliced text data\n",
        "2. ‚úÖ Built character vocabulary\n",
        "3. ‚úÖ Created Dataset and DataLoader\n",
        "4. ‚úÖ Defined LSTM architecture\n",
        "5. ‚úÖ Trained the model\n",
        "6. ‚úÖ Generated new text\n",
        "\n",
        "**Next:** Document your learnings in **Notebook 99 (Lab Notes)**!\n",
        "\n",
        "---\n",
        "\n",
        "*This is honest work. Now go forth and generate!* üöÄ\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Codecademy ML",
      "language": "python",
      "name": "codeacademy"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
