{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 06 ‚Äî Generate Text\n",
        "## Sampling with the Trained LSTM\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Concept Primer\n",
        "\n",
        "### How Text Generation Works\n",
        "\n",
        "Generation is an **autoregressive loop**:\n",
        "\n",
        "```\n",
        "1. Start with a prompt: \"You will rejoice to hear\"\n",
        "2. Feed prompt through model ‚Üí get logits for next char\n",
        "3. Pick next char (greedy: argmax of logits)\n",
        "4. Append char to sequence\n",
        "5. Feed updated sequence ‚Üí get logits for next char\n",
        "6. Repeat until we have 500 characters\n",
        "```\n",
        "\n",
        "### Generation vs. Training\n",
        "\n",
        "| Aspect | Training | Generation |\n",
        "|--------|----------|------------|\n",
        "| **Goal** | Learn from data | Produce new text |\n",
        "| **Mode** | `model.train()` | `model.eval()` |\n",
        "| **Gradients** | Needed | `torch.no_grad()` |\n",
        "| **Input** | Real text batches | Generated chars |\n",
        "| **Output** | Loss | New characters |\n",
        "\n",
        "### Greedy Sampling\n",
        "\n",
        "**Argmax**: Always pick the most likely character.\n",
        "\n",
        "```python\n",
        "logits = model(...)  # [1, vocab_size]\n",
        "next_id = torch.argmax(logits).item()\n",
        "```\n",
        "\n",
        "**Pros**: Simple, deterministic  \n",
        "**Cons**: Repetitive, no creativity\n",
        "\n",
        "**Alternative**: Temperature sampling (adds randomness) ‚Äî left as an extension.\n",
        "\n",
        "### States in Generation\n",
        "\n",
        "Unlike training (batch-level states), generation:\n",
        "- Uses **single batch size = 1**\n",
        "- **Carries states** across time steps (maintains context)\n",
        "- Feeds one character at a time\n",
        "\n",
        "### What Breaks If We Skip This?\n",
        "\n",
        "- No `eval()` = dropout/batchnorm behave incorrectly\n",
        "- Gradients tracked = slow + memory leak\n",
        "- Wrong prompt tokenization = crashes or gibberish\n",
        "\n",
        "### Shapes During Generation\n",
        "\n",
        "| Step | Shape |\n",
        "|------|-------|\n",
        "| **Prompt IDs** | `[1, prompt_length]` |\n",
        "| **Single char input** | `[1, 1]` |\n",
        "| **Logits** | `[1, vocab_size]` |\n",
        "| **States (h, c)** | `[1, 1, 96]` each |\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Objectives\n",
        "\n",
        "By the end of this notebook, you should:\n",
        "\n",
        "- [ ] Load the trained model weights\n",
        "- [ ] Set the model to `eval()` mode\n",
        "- [ ] Define a starting prompt: `\"You will rejoice to hear\"`\n",
        "- [ ] Tokenize the prompt to IDs\n",
        "- [ ] Initialize states for batch size = 1\n",
        "- [ ] Implement generation loop to produce 500 characters\n",
        "- [ ] Decode IDs back to text and print\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéì Acceptance Criteria\n",
        "\n",
        "**You pass this notebook when:**\n",
        "\n",
        "‚úÖ 500 characters of generated text print without errors  \n",
        "‚úÖ Generated text looks vaguely Frankenstein-ish (Gothic, archaic style)  \n",
        "‚úÖ You can explain the difference between greedy and temperature sampling\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 0: Setup ‚Äî Load Data, Model, Weights\n",
        "\n",
        "**Load vocab mappings, define model, load trained weights**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# === Load vocabulary mappings (from notebook 02) ===\n",
        "with open('../datasets/frankenstein.txt', 'r', encoding='utf-8') as f:\n",
        "    frankenstein = f.read()\n",
        "    \n",
        "first_letter_text = frankenstein[1380:8230]\n",
        "tokenized_text = list(first_letter_text)\n",
        "unique_char_tokens = sorted(set(tokenized_text))\n",
        "c2ix = {char: idx for idx, char in enumerate(unique_char_tokens)}\n",
        "ix2c = {idx: char for char, idx in c2ix.items()}\n",
        "vocab_size = len(c2ix)\n",
        "\n",
        "print(f\"Vocabulary loaded: {vocab_size} unique characters\")\n",
        "\n",
        "# === Define Model (same as before) ===\n",
        "class CharacterLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=48, hidden_size=96):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "        self.hidden_size = hidden_size\n",
        "    \n",
        "    def forward(self, x, states):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, new_states = self.lstm(embedded, states)\n",
        "        logits = self.fc(lstm_out)\n",
        "        logits_flat = logits.view(-1, logits.size(-1))\n",
        "        return logits_flat, new_states\n",
        "    \n",
        "    def init_state(self, batch_size):\n",
        "        h0 = torch.zeros(1, batch_size, self.hidden_size)\n",
        "        c0 = torch.zeros(1, batch_size, self.hidden_size)\n",
        "        return (h0, c0)\n",
        "\n",
        "# === Instantiate and load trained weights ===\n",
        "model = CharacterLSTM(vocab_size)\n",
        "model.load_state_dict(torch.load('trained_lstm_model.pth'))\n",
        "model.eval()  # Set to evaluation mode\n",
        "\n",
        "print(\"Model loaded and set to eval() mode\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 1: Define Prompt and Tokenize\n",
        "\n",
        "**Hint:**  \n",
        "Convert prompt string ‚Üí list of char IDs.\n",
        "\n",
        "**Steps:**\n",
        "1. Define `starting_prompt = \"You will rejoice to hear\"`\n",
        "2. Convert to list of IDs: `[c2ix[char] for char in starting_prompt]`\n",
        "3. Convert to tensor: `torch.tensor(..., dtype=torch.long).unsqueeze(0)`\n",
        "   - `unsqueeze(0)` adds batch dimension: `[prompt_length]` ‚Üí `[1, prompt_length]`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Define and tokenize the starting prompt\n",
        "# starting_prompt = \"You will rejoice to hear\"\n",
        "# prompt_ids = [c2ix[char] for char in starting_prompt]\n",
        "# prompt_tensor = torch.tensor(prompt_ids, dtype=torch.long).unsqueeze(0)  # [1, prompt_length]\n",
        "\n",
        "starting_prompt = None  # Replace\n",
        "prompt_tensor = None  # Replace\n",
        "\n",
        "if starting_prompt and prompt_tensor is not None:\n",
        "    print(f\"Prompt: '{starting_prompt}'\")\n",
        "    print(f\"Prompt tensor shape: {prompt_tensor.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 2: Warm Up States with Prompt\n",
        "\n",
        "**Hint:**  \n",
        "Feed the prompt through the model to initialize states.\n",
        "\n",
        "**Steps:**\n",
        "1. Initialize states: `states = model.init_state(1)`\n",
        "2. With `torch.no_grad():`\n",
        "3. Feed prompt: `logits, states = model(prompt_tensor, states)`\n",
        "4. Get last logits: `last_logits = logits[-1:]`  (shape `[1, vocab_size]`)\n",
        "\n",
        "**Why this step?**  \n",
        "The prompt \"primes\" the model with context. The resulting states carry memory of \"You will rejoice to hear\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Feed prompt to warm up states\n",
        "# states = model.init_state(1)\n",
        "# \n",
        "# with torch.no_grad():\n",
        "#     logits, states = model(prompt_tensor, states)\n",
        "#     last_logits = logits[-1:]  # Last time step logits\n",
        "\n",
        "states = None  # Replace\n",
        "last_logits = None  # Replace\n",
        "\n",
        "if states and last_logits is not None:\n",
        "    print(f\"States warmed up. Last logits shape: {last_logits.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 3: Generation Loop\n",
        "\n",
        "**Hint:**  \n",
        "Loop 500 times, generating one character per iteration.\n",
        "\n",
        "**Structure:**\n",
        "```python\n",
        "generated_ids = []\n",
        "num_generated_chars = 500\n",
        "\n",
        "with torch.no_grad():\n",
        "    for _ in range(num_generated_chars):\n",
        "        # 1. Argmax to get next char ID\n",
        "        next_id = torch.argmax(last_logits).item()\n",
        "        generated_ids.append(next_id)\n",
        "        \n",
        "        # 2. Prepare next input: shape [1, 1]\n",
        "        next_input = torch.tensor([[next_id]], dtype=torch.long)\n",
        "        \n",
        "        # 3. Forward pass\n",
        "        logits, states = model(next_input, states)\n",
        "        last_logits = logits[-1:]\n",
        "```\n",
        "\n",
        "**Key details:**\n",
        "- `torch.argmax(last_logits)` picks most likely char\n",
        "- `.item()` converts tensor to Python int\n",
        "- `[[next_id]]` creates shape `[1, 1]`\n",
        "- States are carried across iterations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Generation loop\n",
        "# generated_ids = []\n",
        "# num_generated_chars = 500\n",
        "# \n",
        "# with torch.no_grad():\n",
        "#     for _ in range(num_generated_chars):\n",
        "#         # Get next char ID (greedy sampling)\n",
        "#         next_id = torch.argmax(last_logits).item()\n",
        "#         generated_ids.append(next_id)\n",
        "#         \n",
        "#         # Prepare next input [1, 1]\n",
        "#         next_input = torch.tensor([[next_id]], dtype=torch.long)\n",
        "#         \n",
        "#         # Forward pass\n",
        "#         logits, states = model(next_input, states)\n",
        "#         last_logits = logits[-1:]\n",
        "\n",
        "generated_ids = []  # Replace with your loop\n",
        "\n",
        "if generated_ids:\n",
        "    print(f\"Generated {len(generated_ids)} character IDs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 4: Decode and Print Generated Text\n",
        "\n",
        "**Hint:**  \n",
        "Convert IDs back to characters using `ix2c`.\n",
        "\n",
        "**Steps:**\n",
        "1. Decode: `generated_text = ''.join([ix2c[id] for id in generated_ids])`\n",
        "2. Combine with prompt: `full_text = starting_prompt + generated_text`\n",
        "3. Print the result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Decode generated IDs to text\n",
        "# generated_text = ''.join([ix2c[id] for id in generated_ids])\n",
        "# full_text = starting_prompt + generated_text\n",
        "\n",
        "# print(\"=\"*80)\n",
        "# print(\"GENERATED TEXT (Prompt + 500 chars):\")\n",
        "# print(\"=\"*80)\n",
        "# print(full_text)\n",
        "# print(\"=\"*80)\n",
        "\n",
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üí≠ Reflection Prompts\n",
        "\n",
        "**Write your observations:**\n",
        "\n",
        "1. **Generated style**: Does the generated text resemble Mary Shelley's style? (sentence structure, word choice, punctuation)\n",
        "\n",
        "2. **Coherence**: Is the text coherent over short spans? Long spans?\n",
        "\n",
        "3. **Repetition**: Do you see any repeated phrases or loops?\n",
        "\n",
        "4. **Greedy vs. Sampling**: What would change if we used temperature sampling instead of argmax?\n",
        "\n",
        "5. **Prompt influence**: How much does the starting prompt affect the generated text?\n",
        "\n",
        "6. **Improvements**: What would make the generation better? (More data? Longer training? Larger model?)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Extensions to Try\n",
        "\n",
        "**Want to explore further?**\n",
        "\n",
        "1. **Temperature Sampling**:\n",
        "   ```python\n",
        "   # Instead of argmax:\n",
        "   probs = torch.softmax(last_logits / temperature, dim=-1)\n",
        "   next_id = torch.multinomial(probs, num_samples=1).item()\n",
        "   ```\n",
        "   - `temperature < 1`: More confident (sharper)\n",
        "   - `temperature > 1`: More random (flatter)\n",
        "\n",
        "2. **Longer Generation**: Try 1000 or 2000 characters\n",
        "\n",
        "3. **Different Prompts**: \"I beheld the wretch\", \"It was a dreary night\"\n",
        "\n",
        "4. **Train on Full Novel**: Remove the slice and train on entire *Frankenstein*\n",
        "\n",
        "5. **Beam Search**: Keep top-k candidates at each step\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìå Key Takeaways\n",
        "\n",
        "- ‚úÖ Generation is autoregressive: each char depends on previous chars\n",
        "- ‚úÖ `model.eval()` and `torch.no_grad()` are essential for inference\n",
        "- ‚úÖ Greedy sampling (argmax) is simple but can be repetitive\n",
        "- ‚úÖ States are carried across generation steps to maintain context\n",
        "- ‚úÖ The prompt \"primes\" the model with initial context\n",
        "- ‚úÖ Decoding: IDs ‚Üí characters using `ix2c`\n",
        "\n",
        "---\n",
        "\n",
        "## üéâ Congratulations!\n",
        "\n",
        "You've completed the full pipeline:\n",
        "1. ‚úÖ Loaded and sliced text data\n",
        "2. ‚úÖ Built character vocabulary\n",
        "3. ‚úÖ Created Dataset and DataLoader\n",
        "4. ‚úÖ Defined LSTM architecture\n",
        "5. ‚úÖ Trained the model\n",
        "6. ‚úÖ Generated new text\n",
        "\n",
        "**Next:** Document your learnings in **Notebook 99 (Lab Notes)**!\n",
        "\n",
        "---\n",
        "\n",
        "*This is honest work. Now go forth and generate!* üöÄ\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
