{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 05 ‚Äî Train the LSTM\n",
        "## Training Loop for 5 Epochs\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Concept Primer\n",
        "\n",
        "### The Training Loop\n",
        "\n",
        "Training a neural network follows this pattern:\n",
        "\n",
        "```\n",
        "FOR each epoch:\n",
        "    FOR each batch in dataloader:\n",
        "        1. Zero gradients (clear previous batch's gradients)\n",
        "        2. Initialize states (h0, c0) for this batch\n",
        "        3. Forward pass: get predictions\n",
        "        4. Compute loss (how wrong are we?)\n",
        "        5. Backward pass: compute gradients\n",
        "        6. Optimizer step: update weights\n",
        "    \n",
        "    Print epoch loss\n",
        "```\n",
        "\n",
        "### Loss Function: CrossEntropyLoss\n",
        "\n",
        "**CrossEntropyLoss** measures how far our predictions are from the true labels.\n",
        "\n",
        "**Input shapes:**\n",
        "- Logits: `[B*T, vocab_size]` ‚Äî raw scores for each character\n",
        "- Labels: `[B*T]` ‚Äî true character IDs\n",
        "\n",
        "**Lower loss = better predictions**\n",
        "\n",
        "### Optimizer: Adam\n",
        "\n",
        "**Adam** is an adaptive learning rate optimizer. It:\n",
        "- Adjusts learning rate per parameter\n",
        "- Uses momentum for smoother updates\n",
        "- Works well with default settings\n",
        "\n",
        "**Learning rate = 0.015**: How big each weight update is.\n",
        "\n",
        "### Why Re-initialize States Per Batch?\n",
        "\n",
        "Two options:\n",
        "1. **Re-init per batch** (simpler): Each batch is independent\n",
        "2. **Carry states across batches** (complex): Requires detaching gradients\n",
        "\n",
        "We use option 1 for simplicity.\n",
        "\n",
        "### What Breaks If We Skip This?\n",
        "\n",
        "- No `zero_grad()` = gradients accumulate incorrectly\n",
        "- Wrong label shape = loss computation fails\n",
        "- No backward = weights never update\n",
        "- No optimizer step = loss never improves\n",
        "\n",
        "### Shapes During Training\n",
        "\n",
        "| Step | Shape |\n",
        "|------|-------|\n",
        "| **Batch features** | `[36, 48]` |\n",
        "| **Batch labels** | `[36, 48]` |\n",
        "| **Labels flattened** | `[36*48] = [1728]` |\n",
        "| **Logits** | `[1728, vocab_size]` |\n",
        "| **Loss** | scalar |\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Objectives\n",
        "\n",
        "By the end of this notebook, you should:\n",
        "\n",
        "- [ ] Load all components from previous notebooks (data, model)\n",
        "- [ ] Instantiate `nn.CrossEntropyLoss()` as the loss function\n",
        "- [ ] Create `Adam` optimizer with `lr=0.015`\n",
        "- [ ] Implement the training loop for 5 epochs\n",
        "- [ ] Print loss per epoch and observe it decreasing\n",
        "- [ ] Save the trained model weights\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéì Acceptance Criteria\n",
        "\n",
        "**You pass this notebook when:**\n",
        "\n",
        "‚úÖ Training runs for 5 epochs without errors  \n",
        "‚úÖ Loss prints after each epoch  \n",
        "‚úÖ Loss generally trends downward (not always monotonic, but overall lower)  \n",
        "‚úÖ Model weights are saved to disk\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 0: Setup ‚Äî Imports and Data/Model Loading\n",
        "\n",
        "**Load everything from previous notebooks**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "\n",
        "# === Load Data (from notebooks 01-02) ===\n",
        "with open('../datasets/frankenstein.txt', 'r', encoding='utf-8') as f:\n",
        "    frankenstein = f.read()\n",
        "    \n",
        "first_letter_text = frankenstein[1380:8230]\n",
        "tokenized_text = list(first_letter_text)\n",
        "unique_char_tokens = sorted(set(tokenized_text))\n",
        "c2ix = {char: idx for idx, char in enumerate(unique_char_tokens)}\n",
        "ix2c = {idx: char for char, idx in c2ix.items()}\n",
        "vocab_size = len(c2ix)\n",
        "tokenized_id_text = [c2ix[char] for char in tokenized_text]\n",
        "\n",
        "print(f\"Data loaded: {len(tokenized_id_text)} IDs, vocab_size={vocab_size}\")\n",
        "\n",
        "# === Define Dataset (from notebook 03) ===\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenized_ids, seq_length):\n",
        "        self.ids = tokenized_ids\n",
        "        self.seq_length = seq_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.ids) - self.seq_length\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        features = self.ids[idx : idx + self.seq_length]\n",
        "        labels = self.ids[idx + 1 : idx + self.seq_length + 1]\n",
        "        return (\n",
        "            torch.tensor(features, dtype=torch.long),\n",
        "            torch.tensor(labels, dtype=torch.long)\n",
        "        )\n",
        "\n",
        "# === Create Dataset & DataLoader ===\n",
        "dataset = TextDataset(tokenized_id_text, seq_length=48)\n",
        "dataloader = DataLoader(dataset, batch_size=36, shuffle=True)\n",
        "\n",
        "print(f\"Dataset: {len(dataset)} samples\")\n",
        "print(f\"DataLoader: {len(dataloader)} batches per epoch\")\n",
        "\n",
        "# === Define Model (from notebook 04) ===\n",
        "class CharacterLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=48, hidden_size=96):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "        self.hidden_size = hidden_size\n",
        "    \n",
        "    def forward(self, x, states):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, new_states = self.lstm(embedded, states)\n",
        "        logits = self.fc(lstm_out)\n",
        "        logits_flat = logits.view(-1, logits.size(-1))\n",
        "        return logits_flat, new_states\n",
        "    \n",
        "    def init_state(self, batch_size):\n",
        "        h0 = torch.zeros(1, batch_size, self.hidden_size)\n",
        "        c0 = torch.zeros(1, batch_size, self.hidden_size)\n",
        "        return (h0, c0)\n",
        "\n",
        "# === Instantiate Model ===\n",
        "lstm_model = CharacterLSTM(vocab_size)\n",
        "print(f\"\\nModel instantiated with {sum(p.numel() for p in lstm_model.parameters()):,} parameters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 1: Define Loss and Optimizer\n",
        "\n",
        "**Hint:**  \n",
        "Use `nn.CrossEntropyLoss()` and `Adam(model.parameters(), lr=0.015)`.\n",
        "\n",
        "**Steps:**\n",
        "1. Create loss function: `criterion = nn.CrossEntropyLoss()`\n",
        "2. Create optimizer: `optimizer = Adam(lstm_model.parameters(), lr=0.015)`\n",
        "\n",
        "**Why CrossEntropyLoss?**  \n",
        "It combines softmax + negative log likelihood ‚Äî perfect for classification tasks (predicting next char).\n",
        "\n",
        "**Why lr=0.015?**  \n",
        "Experimentation shows this works well for this small dataset. Too high = unstable, too low = slow learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Define loss function and optimizer\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = Adam(lstm_model.parameters(), lr=0.015)\n",
        "\n",
        "criterion = None  # Replace this line\n",
        "optimizer = None  # Replace this line\n",
        "\n",
        "if criterion and optimizer:\n",
        "    print(\"Loss function: CrossEntropyLoss\")\n",
        "    print(\"Optimizer: Adam with lr=0.015\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 2: Implement the Training Loop\n",
        "\n",
        "**Hint:**  \n",
        "Nested loops: outer for epochs, inner for batches.\n",
        "\n",
        "**Structure:**\n",
        "```python\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for batch_features, batch_labels in dataloader:\n",
        "        # 1. Zero gradients\n",
        "        # 2. Initialize states\n",
        "        # 3. Forward pass\n",
        "        # 4. Compute loss (flatten labels first!)\n",
        "        # 5. Backward\n",
        "        # 6. Optimizer step\n",
        "        # 7. Accumulate loss\n",
        "    \n",
        "    # Print average epoch loss\n",
        "```\n",
        "\n",
        "**Key details:**\n",
        "- **Flatten labels**: `batch_labels.view(-1)` ‚Üí `[B*T]`\n",
        "- **Get batch size**: `batch_features.size(0)`\n",
        "- **Accumulate loss**: Use `loss.item()` to get scalar value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Training loop for 5 epochs\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     epoch_loss = 0\n",
        "#     \n",
        "#     for batch_features, batch_labels in dataloader:\n",
        "#         # Get batch size\n",
        "#         batch_size = batch_features.size(0)\n",
        "#         \n",
        "#         # 1. Zero gradients\n",
        "#         optimizer.zero_grad()\n",
        "#         \n",
        "#         # 2. Initialize states for this batch\n",
        "#         states = lstm_model.init_state(batch_size)\n",
        "#         \n",
        "#         # 3. Forward pass\n",
        "#         logits, new_states = lstm_model(batch_features, states)\n",
        "#         \n",
        "#         # 4. Compute loss\n",
        "#         # Flatten labels to [B*T]\n",
        "#         labels_flat = batch_labels.view(-1)\n",
        "#         loss = criterion(logits, labels_flat)\n",
        "#         \n",
        "#         # 5. Backward pass\n",
        "#         loss.backward()\n",
        "#         \n",
        "#         # 6. Optimizer step\n",
        "#         optimizer.step()\n",
        "#         \n",
        "#         # 7. Accumulate loss\n",
        "#         epoch_loss += loss.item()\n",
        "#     \n",
        "#     # Print average loss for this epoch\n",
        "#     avg_loss = epoch_loss / len(dataloader)\n",
        "#     print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Your code here\n",
        "print(\"\\\\nTraining complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 3: Save the Trained Model\n",
        "\n",
        "**Hint:**  \n",
        "Use `torch.save(model.state_dict(), path)`.\n",
        "\n",
        "**Steps:**\n",
        "1. Save model weights: `torch.save(lstm_model.state_dict(), 'trained_lstm_model.pth')`\n",
        "2. Print confirmation\n",
        "\n",
        "**Why save?**  \n",
        "So you can load the trained weights in notebook 06 for text generation without retraining.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Save the trained model\n",
        "# torch.save(lstm_model.state_dict(), 'trained_lstm_model.pth')\n",
        "# print(\"Model saved to 'trained_lstm_model.pth'\")\n",
        "\n",
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üí≠ Reflection Prompts\n",
        "\n",
        "**Write your observations:**\n",
        "\n",
        "1. **Loss trend**: Did your loss decrease over epochs? By how much?\n",
        "\n",
        "2. **What is the loss measuring**: In plain words, what does the loss value represent?\n",
        "\n",
        "3. **Why zero_grad()**: What would happen if you forgot to call `optimizer.zero_grad()`?\n",
        "\n",
        "4. **Batch size impact**: How would changing batch_size from 36 to 12 or 72 affect training?\n",
        "\n",
        "5. **Learning rate**: What would happen with lr=0.0001 (too small) or lr=0.5 (too large)?\n",
        "\n",
        "6. **Epoch count**: Is 5 epochs enough? How would you decide?\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Next Steps\n",
        "\n",
        "Once you've completed training and loss is decreasing:\n",
        "\n",
        "‚û°Ô∏è **Move to Notebook 06**: Generate text with the trained model\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Key Takeaways\n",
        "\n",
        "- ‚úÖ Training loop: zero_grad ‚Üí forward ‚Üí loss ‚Üí backward ‚Üí step\n",
        "- ‚úÖ CrossEntropyLoss measures prediction error\n",
        "- ‚úÖ Adam optimizer updates weights to minimize loss\n",
        "- ‚úÖ States are re-initialized per batch for simplicity\n",
        "- ‚úÖ Label shape must be `[B*T]` for CrossEntropyLoss\n",
        "- ‚úÖ Loss should trend downward (not always monotonic)\n",
        "- ‚úÖ Save model weights to reuse them later\n",
        "\n",
        "---\n",
        "\n",
        "*Next up: Using the trained model to generate Frankenstein-style text!*\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
