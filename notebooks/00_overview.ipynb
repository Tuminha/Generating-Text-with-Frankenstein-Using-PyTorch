{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 00 â€” Map of the Journey\n",
        "## Character-Level Language Modeling with LSTM\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ¯ Concept Primer\n",
        "\n",
        "### What is Character-Level Language Modeling?\n",
        "\n",
        "A **character-level language model** predicts the next character in a sequence, given all previous characters. Unlike word-level models, we work directly with individual letters, spaces, and punctuation marks.\n",
        "\n",
        "**Example:**\n",
        "- Input: `\"You will rejoice to hea\"`\n",
        "- Model predicts: `\"r\"` (the next character)\n",
        "\n",
        "### Why LSTM?\n",
        "\n",
        "**LSTM (Long Short-Term Memory)** networks maintain memory through *gates* that control what information to keep, forget, or output:\n",
        "- **Forget gate**: What to throw away from memory\n",
        "- **Input gate**: What new information to store\n",
        "- **Output gate**: What to output based on memory\n",
        "\n",
        "This memory mechanism lets the model learn long-range patterns like \"an open quote needs a close quote\" or \"if then needs an else.\"\n",
        "\n",
        "### What Breaks If We Skip This?\n",
        "\n",
        "Without proper understanding of the pipeline, you'll struggle to debug shape mismatches, loss explosions, or nonsensical generated text.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Š The Complete Pipeline\n",
        "\n",
        "```\n",
        "1. LOAD TEXT\n",
        "   â””â”€> Raw string: \"You will rejoice...\"\n",
        "\n",
        "2. TOKENIZE (Character-Level)\n",
        "   â””â”€> List of chars: ['Y', 'o', 'u', ' ', 'w', 'i', 'l', 'l', ...]\n",
        "\n",
        "3. BUILD VOCABULARY\n",
        "   â”œâ”€> c2ix: {'a': 0, 'b': 1, ..., 'z': 25, ' ': 26, ...}\n",
        "   â””â”€> ix2c: {0: 'a', 1: 'b', ..., 26: ' ', ...}\n",
        "\n",
        "4. CONVERT TO IDs\n",
        "   â””â”€> [34, 14, 20, 26, 22, 8, 11, 11, ...]\n",
        "\n",
        "5. CREATE SLIDING WINDOWS\n",
        "   â”œâ”€> Features: [34, 14, 20, 26, 22, 8, 11, 11]  (input sequence)\n",
        "   â””â”€> Labels:   [14, 20, 26, 22, 8, 11, 11, 32]  (shifted by 1)\n",
        "\n",
        "6. DATALOADER (Batching)\n",
        "   â””â”€> Shape: [batch_size, seq_length]\n",
        "\n",
        "7. LSTM MODEL\n",
        "   â”œâ”€> Embedding: [B, T] â†’ [B, T, embedding_dim]\n",
        "   â”œâ”€> LSTM: [B, T, embedding_dim] â†’ [B, T, hidden_size]\n",
        "   â””â”€> Linear: [B, T, hidden_size] â†’ [B, T, vocab_size]\n",
        "\n",
        "8. LOSS (CrossEntropyLoss)\n",
        "   â”œâ”€> Logits: [B*T, vocab_size]\n",
        "   â””â”€> Labels: [B*T]\n",
        "\n",
        "9. TRAINING LOOP\n",
        "   â””â”€> Forward â†’ Loss â†’ Backward â†’ Optimizer Step\n",
        "\n",
        "10. GENERATION (Sampling)\n",
        "    â”œâ”€> Start with prompt: \"You will rejoice to hear\"\n",
        "    â”œâ”€> Loop: feed last char â†’ get logits â†’ sample â†’ append\n",
        "    â””â”€> Generate 500 new characters\n",
        "```\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âœ… Objectives\n",
        "\n",
        "By the end of this overview, you should be able to:\n",
        "\n",
        "- [ ] Explain what \"character-level\" means and why it's simpler than word-level\n",
        "- [ ] Describe the three LSTM gates and their purposes\n",
        "- [ ] Trace the flow from raw text â†’ tokens â†’ IDs â†’ batches â†’ model â†’ loss\n",
        "- [ ] Understand why we need both `c2ix` (encoding) and `ix2c` (decoding)\n",
        "- [ ] Recognize tensor shapes at each pipeline stage\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ“ Acceptance Criteria\n",
        "\n",
        "**You pass this notebook when:**\n",
        "\n",
        "âœ… You can narrate the full pipeline (steps 1-10 above) in under 90 seconds  \n",
        "âœ… You can explain why labels are \"features shifted by 1\"  \n",
        "âœ… You understand why we reshape to `[B*T, vocab_size]` for CrossEntropyLoss  \n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§  Key Concepts to Remember\n",
        "\n",
        "### Why Character-Level?\n",
        "- **Small vocabulary**: ~50-100 unique characters vs. 10,000+ words\n",
        "- **No tokenizer complexity**: No need for BPE, WordPiece, etc.\n",
        "- **Directly teaches sequence modeling**: Clear input/output alignment\n",
        "\n",
        "### Shape Notation Used Throughout\n",
        "- `B` = batch size (e.g., 36)\n",
        "- `T` = sequence length / time steps (e.g., 48)\n",
        "- `E` = embedding dimension (e.g., 48)\n",
        "- `H` = hidden size (e.g., 96)\n",
        "- `V` = vocab size (e.g., 50-80 depending on unique chars)\n",
        "\n",
        "### The Training Signal\n",
        "**Predict the next character** â€” that's it! The model learns patterns like:\n",
        "- After `\"th\"`, `\"e\"` is likely\n",
        "- Open quotes need close quotes\n",
        "- Sentence structure and punctuation patterns\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ’­ Reflection Prompt\n",
        "\n",
        "**In your own words:**\n",
        "\n",
        "1. What is a \"hidden state\" in an LSTM?  \n",
        "   *(Write your one-sentence definition here before moving to the next notebook)*\n",
        "\n",
        "2. Why do we need both `h` (hidden state) and `c` (cell state) in LSTM?\n",
        "\n",
        "3. What's the difference between *training* (using real next chars) and *generation* (sampling predicted chars)?\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸš€ Next Steps\n",
        "\n",
        "Proceed to:\n",
        "- **Notebook 01**: Load and slice the Frankenstein text\n",
        "- **Notebook 02**: Build character vocabulary and convert text to IDs\n",
        "- **Notebook 03**: Create Dataset and DataLoader for batching\n",
        "- **Notebook 04**: Define the LSTM model architecture\n",
        "- **Notebook 05**: Train the model\n",
        "- **Notebook 06**: Generate new text\n",
        "- **Notebook 99**: Lab notes for reflections\n",
        "\n",
        "---\n",
        "\n",
        "*Remember: These notebooks contain TODOs and hints, not complete solutions. Learning happens when you write the code yourself!* ðŸŽ“\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
