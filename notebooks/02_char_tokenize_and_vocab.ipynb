{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 ‚Äî Character Tokenization & Vocabulary\n",
        "## Build the Character-to-ID Mapping\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Concept Primer\n",
        "\n",
        "### What is Tokenization?\n",
        "\n",
        "**Tokenization** breaks text into units (tokens). For us, each token = one character.\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "Text:    \"Hi!\"\n",
        "Tokens:  ['H', 'i', '!']\n",
        "```\n",
        "\n",
        "### Why Character-Level Tokens?\n",
        "\n",
        "- **Small vocabulary**: ~50-80 unique characters (letters, punctuation, spaces)\n",
        "- **No unknown words**: Every possible character is in the vocab\n",
        "- **Simplicity**: No need for complex tokenizers like BPE or WordPiece\n",
        "\n",
        "### Building the Vocabulary\n",
        "\n",
        "We need **two dictionaries**:\n",
        "\n",
        "1. **`c2ix`** (char ‚Üí index): Encode characters to integers for the model  \n",
        "   `{'a': 0, 'b': 1, ..., ' ': 26, ...}`\n",
        "\n",
        "2. **`ix2c`** (index ‚Üí char): Decode integers back to characters for text generation  \n",
        "   `{0: 'a', 1: 'b', ..., 26: ' ', ...}`\n",
        "\n",
        "### What Breaks If We Skip This?\n",
        "\n",
        "- No mapping = can't convert text to numbers\n",
        "- Inconsistent ordering = non-reproducible results\n",
        "- Missing characters = crashes during generation\n",
        "\n",
        "### Shapes\n",
        "- **Input**: `first_letter_text` (string, ~6,850 chars)\n",
        "- **Tokens**: List of 6,850 characters\n",
        "- **Unique chars**: ~50-80 (sorted for consistency)\n",
        "- **ID sequence**: List of 6,850 integers\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Objectives\n",
        "\n",
        "By the end of this notebook, you should:\n",
        "\n",
        "- [ ] Convert `first_letter_text` into a list of characters ‚Üí `tokenized_text`\n",
        "- [ ] Extract all unique characters and sort them ‚Üí `unique_char_tokens`\n",
        "- [ ] Build `c2ix` dictionary (char ‚Üí index)\n",
        "- [ ] Build `ix2c` dictionary (index ‚Üí char)\n",
        "- [ ] Calculate `vocab_size`\n",
        "- [ ] Convert the full text to IDs ‚Üí `tokenized_id_text`\n",
        "- [ ] Verify by printing first 100 IDs\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéì Acceptance Criteria\n",
        "\n",
        "**You pass this notebook when:**\n",
        "\n",
        "‚úÖ `vocab_size` prints (should be ~50-80)  \n",
        "‚úÖ First 100 IDs of `tokenized_id_text` display correctly  \n",
        "‚úÖ You can manually check: `c2ix['a']` and `ix2c[0]` work as expected  \n",
        "‚úÖ You understand why we sort the unique characters\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 0: Load the Data\n",
        "\n",
        "**Note:** Copy your loading code from Notebook 01, or re-run it here.\n",
        "\n",
        "**You need:**\n",
        "- `first_letter_text` variable populated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Load first_letter_text from Notebook 01\n",
        "# (Copy your code from 01, or simply re-run those cells)\n",
        "\n",
        "with open('../datasets/frankenstein.txt', 'r', encoding='utf-8') as f:\n",
        "    frankenstein = f.read()\n",
        "    \n",
        "first_letter_text = frankenstein[1380:8230]\n",
        "\n",
        "print(f\"Loaded {len(first_letter_text)} characters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 1: Tokenize Into Characters\n",
        "\n",
        "**Hint:**  \n",
        "A string in Python is already iterable. Convert it to a list.\n",
        "\n",
        "**Steps:**\n",
        "1. Use `list(first_letter_text)` to convert the string into a list of characters\n",
        "2. Assign to `tokenized_text`\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "text = \"Hi!\"\n",
        "tokens = list(text)  # ['H', 'i', '!']\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Tokenize the text into characters\n",
        "# tokenized_text = list(first_letter_text)\n",
        "\n",
        "tokenized_text = None  # Replace this line\n",
        "\n",
        "# Verify\n",
        "if tokenized_text:\n",
        "    print(f\"Total tokens: {len(tokenized_text)}\")\n",
        "    print(f\"First 50 tokens: {tokenized_text[:50]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 2: Extract Unique Characters (Sorted)\n",
        "\n",
        "**Hint:**  \n",
        "Use `set()` to get unique characters, then `sorted()` to sort alphabetically.\n",
        "\n",
        "**Why sort?**  \n",
        "Sorting ensures consistent IDs across runs. If we don't sort, `set()` order is unpredictable.\n",
        "\n",
        "**Steps:**\n",
        "1. Convert `tokenized_text` to a set to get unique characters\n",
        "2. Sort it with `sorted()`\n",
        "3. Assign to `unique_char_tokens`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Get unique characters and sort them\n",
        "# unique_char_tokens = sorted(set(tokenized_text))\n",
        "\n",
        "unique_char_tokens = None  # Replace this line\n",
        "\n",
        "# Verify\n",
        "if unique_char_tokens:\n",
        "    print(f\"Unique characters: {len(unique_char_tokens)}\")\n",
        "    print(f\"Vocabulary: {unique_char_tokens}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 3: Build c2ix (char ‚Üí index)\n",
        "\n",
        "**Hint:**  \n",
        "Use dictionary comprehension with `enumerate()`.\n",
        "\n",
        "**Steps:**\n",
        "1. Enumerate through `unique_char_tokens` to get (index, char) pairs\n",
        "2. Create a dict mapping char ‚Üí index\n",
        "3. Assign to `c2ix`\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "chars = ['a', 'b', 'c']\n",
        "c2ix = {char: idx for idx, char in enumerate(chars)}\n",
        "# Result: {'a': 0, 'b': 1, 'c': 2}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Build char-to-index dictionary\n",
        "# c2ix = {char: idx for idx, char in enumerate(unique_char_tokens)}\n",
        "\n",
        "c2ix = None  # Replace this line\n",
        "\n",
        "# Verify\n",
        "if c2ix:\n",
        "    print(f\"Sample mappings from c2ix:\")\n",
        "    for char in [' ', 'a', 'e', 't', '.']:\n",
        "        if char in c2ix:\n",
        "            print(f\"  '{char}' ‚Üí {c2ix[char]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 4: Build ix2c (index ‚Üí char)\n",
        "\n",
        "**Hint:**  \n",
        "Reverse the c2ix dictionary.\n",
        "\n",
        "**Steps:**\n",
        "1. Use dictionary comprehension: swap keys and values\n",
        "2. Assign to `ix2c`\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "c2ix = {'a': 0, 'b': 1}\n",
        "ix2c = {idx: char for char, idx in c2ix.items()}\n",
        "# Result: {0: 'a', 1: 'b'}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Build index-to-char dictionary\n",
        "# ix2c = {idx: char for char, idx in c2ix.items()}\n",
        "\n",
        "ix2c = None  # Replace this line\n",
        "\n",
        "# Verify\n",
        "if ix2c:\n",
        "    print(f\"Sample mappings from ix2c:\")\n",
        "    for idx in [0, 1, 2, 3, 4]:\n",
        "        if idx in ix2c:\n",
        "            print(f\"  {idx} ‚Üí '{ix2c[idx]}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 5: Calculate vocab_size\n",
        "\n",
        "**Hint:**  \n",
        "The vocabulary size is simply the number of unique characters.\n",
        "\n",
        "**Steps:**\n",
        "1. Use `len(c2ix)` or `len(unique_char_tokens)`\n",
        "2. Assign to `vocab_size`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Calculate vocabulary size\n",
        "# vocab_size = len(c2ix)\n",
        "\n",
        "vocab_size = None  # Replace this line\n",
        "\n",
        "# Verify\n",
        "if vocab_size:\n",
        "    print(f\"Vocabulary size: {vocab_size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù TODO 6: Convert Text to IDs\n",
        "\n",
        "**Hint:**  \n",
        "Map each character through the `c2ix` dictionary.\n",
        "\n",
        "**Steps:**\n",
        "1. Use a list comprehension to map each char in `tokenized_text` to its ID\n",
        "2. Look up each char: `c2ix[char]`\n",
        "3. Assign to `tokenized_id_text`\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "text = ['h', 'i']\n",
        "c2ix = {'h': 5, 'i': 8}\n",
        "ids = [c2ix[char] for char in text]\n",
        "# Result: [5, 8]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Convert all characters to IDs\n",
        "# tokenized_id_text = [c2ix[char] for char in tokenized_text]\n",
        "\n",
        "tokenized_id_text = None  # Replace this line\n",
        "\n",
        "# Verify\n",
        "if tokenized_id_text:\n",
        "    print(f\"Total IDs: {len(tokenized_id_text)}\")\n",
        "    print(f\"First 100 IDs:\\n{tokenized_id_text[:100]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üí≠ Reflection Prompts\n",
        "\n",
        "**Write your observations:**\n",
        "\n",
        "1. **Non-letter characters**: Which non-letter characters appear in your vocabulary? (spaces, punctuation, newlines?)\n",
        "\n",
        "2. **Most common characters**: From the first 100 IDs, which IDs (and their corresponding characters) appear most frequently?\n",
        "\n",
        "3. **Why sorting matters**: What would happen if we didn't sort `unique_char_tokens`?\n",
        "\n",
        "4. **Encoding vs. Decoding**: Why do we need both `c2ix` and `ix2c`? When do we use each?\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Next Steps\n",
        "\n",
        "Once you've completed all TODOs and verified your vocab size:\n",
        "\n",
        "‚û°Ô∏è **Move to Notebook 03**: Creating Dataset & DataLoader with sliding windows\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Key Takeaways\n",
        "\n",
        "- ‚úÖ Character tokenization = `list(text)`\n",
        "- ‚úÖ Vocab = sorted unique characters for consistency\n",
        "- ‚úÖ `c2ix` encodes (text ‚Üí model), `ix2c` decodes (model ‚Üí text)\n",
        "- ‚úÖ IDs are what the model actually processes\n",
        "- ‚úÖ Every character (including spaces, punctuation, newlines) gets an ID\n",
        "\n",
        "---\n",
        "\n",
        "*Next up: We'll create sliding windows of these IDs to form training examples!*\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
