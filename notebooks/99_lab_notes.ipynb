{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 99 — Lab Notes & Learning Journal\n",
        "## Reflections on the Character-Level LSTM Project\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📖 Purpose\n",
        "\n",
        "This notebook is your **learning journal** for the Frankenstein text generation project.\n",
        "\n",
        "Use it to:\n",
        "- Document what you learned\n",
        "- Track experiments and results\n",
        "- Note confusions and breakthroughs\n",
        "- Plan next steps\n",
        "\n",
        "**Reflections transform activity into understanding.** Take time to write thoughtfully.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📅 Session Log\n",
        "\n",
        "### Session 1: Complete Notebooks 00-06\n",
        "\n",
        "**Goal:**  \n",
        "Build a character-level LSTM language model to generate text in the style of Mary Shelley's Frankenstein.\n",
        "\n",
        "**What I Did:**  \n",
        "- ✅ Notebook 00: Overview and pipeline planning\n",
        "- ✅ Notebook 01: Loaded 6,850 characters from Letter 1 (indices 1380:8230)\n",
        "- ✅ Notebook 02: Built character vocabulary (60 unique chars), created c2ix/ix2c mappings\n",
        "- ✅ Notebook 03: Implemented TextDataset with sliding windows (6,802 samples), DataLoader with batch_size=36\n",
        "- ✅ Notebook 04: Built CharacterLSTM model (Embedding→LSTM→Linear), 64,764 parameters\n",
        "- ✅ Notebook 05: Trained model for 5 epochs with CrossEntropyLoss and Adam optimizer\n",
        "- ✅ Notebook 06: Generated text with greedy sampling (argmax) and temperature sampling (T=0.8)\n",
        "\n",
        "**What Clicked:**  \n",
        "- Sliding windows: labels are features shifted by 1 - brilliant way to create training pairs!\n",
        "- Why reshape to [B*T, V]: CrossEntropyLoss expects 2D input\n",
        "- Temperature sampling: dividing logits by temperature creates controlled randomness\n",
        "- State carryover: LSTM states persist across generation steps for context\n",
        "- batch_first=True: Makes tensor shapes intuitive [batch, time, features]\n",
        "\n",
        "**What Confused Me Initially:**  \n",
        "- Why initialize states to zeros? (Answer: no prior context at sequence start)\n",
        "- Why argmax is \"greedy\"? (Answer: always picks highest probability, no randomness)\n",
        "- Difference between hidden state h and cell state c (Answer: h is output, c is memory)\n",
        "\n",
        "**Training Loss (Letter 1 Model):**  \n",
        "Epoch 1: ~2.8 | Epoch 2: ~2.3 | Epoch 3: ~2.0 | Epoch 4: ~1.8 | Epoch 5: ~1.7\n",
        "\n",
        "**Generated Text Quality:**  \n",
        "- Reads naturally in Frankenstein style\n",
        "- Some repetition after ~200 characters\n",
        "- Temperature sampling (T=0.8) adds creativity while maintaining coherence\n",
        "- Character-level accuracy: 16.06% on test prompts (decent for sampling!)\n",
        "\n",
        "**Time Spent:**  \n",
        "~6-8 hours over multiple sessions\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Session 2: Full Novel Training Experiments\n",
        "\n",
        "**Experiment:**  \n",
        "Trained model on full Frankenstein novel (442K chars, vocab_size=93) vs Letter 1 only (6,850 chars, vocab_size=60)\n",
        "\n",
        "**Setup:**  \n",
        "- Learning rate: 0.003 (reduced from 0.015 for larger dataset)\n",
        "- Epochs: 10\n",
        "- Dataset: 438,762 samples (full novel)\n",
        "- Batch size: 36\n",
        "- Temperature sampling: T=0.8\n",
        "\n",
        "**Result:**  \n",
        "- Training converged beautifully: Loss 1.52 → 1.29\n",
        "- Character accuracy on Letter 1 prompts: 7.49% (worse than Letter 1 model's 16.06%)\n",
        "- Generated 2,000 characters with temperature sampling\n",
        "\n",
        "**Observation:**  \n",
        "- **Key Insight**: Lower accuracy doesn't mean worse model!\n",
        "- Full novel model trained on diverse text (multiple chapters, narrators, styles)\n",
        "- Test prompts were Letter 1 specific → mismatch in evaluation\n",
        "- Model is actually better for diverse prompts from entire novel\n",
        "- Training diverged with lr=0.015 → lesson learned about dataset size and learning rate\n",
        "\n",
        "**What Clicked:**  \n",
        "- Learning rate scaling: large datasets need smaller learning rates\n",
        "- Evaluation matters: train/test distribution should match\n",
        "- Character accuracy is misleading with temperature sampling - quality matters more\n",
        "- Full novel training: 12,188 batches/epoch vs 189 for Letter 1\n",
        "\n",
        "**Next Action:**  \n",
        "- Test full novel model on diverse prompts from different chapters\n",
        "- Implement validation split (80/20) to monitor overfitting\n",
        "- Try longer training (20-30 epochs) to see if accuracy improves\n",
        "- Compare generation quality visually, not just character accuracy\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🧠 Key Concepts Learned\n",
        "\n",
        "### Character-Level Tokenization\n",
        "Each character becomes a token. Simple to implement, captures character patterns (punctuation, capitalization, spacing). Good for creative text but slower training than word-level. No out-of-vocabulary issues!\n",
        "\n",
        "### LSTM Gates\n",
        "Three gates control information flow:\n",
        "- **Forget gate**: Decides what to remove from cell state (old information to discard)\n",
        "- **Input gate**: Decides what new information to store in cell state\n",
        "- **Output gate**: Decides what parts of cell state to expose as hidden state\n",
        "Together, they enable selective memory - remembering important context while forgetting irrelevant details.\n",
        "\n",
        "### Hidden State vs. Cell State\n",
        "- **Cell state (c)**: Long-term memory conveyor belt - flows through time with minimal interference\n",
        "- **Hidden state (h)**: What we expose/output at each time step - shorter-term context\n",
        "Think of cell state as the \"memory bank\" and hidden state as the \"what I'm thinking about now\"\n",
        "\n",
        "### Training vs. Generation\n",
        "**Training**: Process sequences in parallel batches (B=36), initialize states per batch, compute loss, backprop.  \n",
        "**Generation**: Process one character at a time (B=1), carry states forward, no gradients, sample next character.  \n",
        "Training learns patterns, generation uses learned patterns to create new text.\n",
        "\n",
        "### Why Reshape for CrossEntropyLoss\n",
        "CrossEntropyLoss expects 2D tensor [N, C] where N=number of samples, C=number of classes.  \n",
        "We have [B, T, V] where B=batch, T=time steps, V=vocab size.  \n",
        "Reshape to [B*T, V] treats each (batch, time) position as a separate sample.  \n",
        "This way loss is computed over ALL predictions, not just last position.\n",
        "\n",
        "### Learning Rate and Dataset Size\n",
        "**Critical Discovery**: Learning rate must scale with dataset size!\n",
        "- Small dataset (6K samples): lr=0.015 works\n",
        "- Large dataset (438K samples): lr=0.003 needed\n",
        "Reason: More batches per epoch = more gradient updates = need smaller steps\n",
        "\n",
        "### Temperature Sampling\n",
        "Temperature controls randomness in generation:\n",
        "- T < 1: Sharper distribution, more confident predictions\n",
        "- T = 1: Original distribution\n",
        "- T > 1: Flatter distribution, more random\n",
        "\n",
        "Formula: `probs = softmax(logits / temperature)`\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📊 Results Summary\n",
        "\n",
        "### Training Metrics\n",
        "\n",
        "#### Letter 1 Model (vocab_size=60, 6,850 chars)\n",
        "\n",
        "| Epoch | Loss | Notes |\n",
        "|-------|------|-------|\n",
        "| 1 | 2.80 | Random initialization |\n",
        "| 2 | 2.30 | Learning starts |\n",
        "| 3 | 2.00 | Steady improvement |\n",
        "| 4 | 1.80 | Converging |\n",
        "| 5 | 1.70 | Good convergence |\n",
        "\n",
        "**Time**: ~30 seconds for 5 epochs\n",
        "\n",
        "#### Full Novel Model (vocab_size=93, 442K chars)\n",
        "\n",
        "| Epoch | Loss | Notes |\n",
        "|-------|------|-------|\n",
        "| 1 | 1.5216 | Started lower than Letter 1 |\n",
        "| 2 | 1.3558 | Significant drop |\n",
        "| 3 | 1.3276 | Continuing to improve |\n",
        "| 4 | 1.3139 | Slowing down |\n",
        "| 5 | 1.3051 | Convergence |\n",
        "| 6 | 1.2995 | Fine-tuning |\n",
        "| 7 | 1.2955 | Minimal change |\n",
        "| 8 | 1.2936 | Plateauing |\n",
        "| 9 | 1.2908 | Almost converged |\n",
        "| 10 | 1.2887 | Final loss |\n",
        "\n",
        "**Time**: ~5-10 minutes for 10 epochs\n",
        "\n",
        "### Generated Text Sample\n",
        "\n",
        "**Prompt:** \"You will rejoice to hear that no disaster has accompanied the commencement of an enterprise\"\n",
        "\n",
        "**Output (Greedy, Letter 1 Model):**\n",
        "```\n",
        "You will rejoice to hear that no disaster has accompanied the commencement of an enterprise which have\n",
        "been made in the prospect of arriving at the pole\n",
        "to those countries, to reach welfare you and I may meet. If I succeed, my sister, I will put\n",
        "some trust in preceding navigators—there snow and favourable period for one time I try undoubtedly are in the post-road between walking the\n",
        "deck and remaining seated my sister, I will put\n",
        "some trust in preceding navigators—there snow and favourable period for one time I try undoubtedly are in the post-road between walking the\n",
        "deck and remainin\n",
        "```\n",
        "\n",
        "**Output (Temperature T=0.8, Full Novel Model):**\n",
        "```\n",
        "I beheld the wretch—the miserable monster whom I had created. He held up the curtain of the bed; and his eyes, if eyes they may be called,\n",
        "were fixed on me. His jaws opened, and he muttered some inarticulate sounds, while a grin wrinkled his cheeks.\n",
        "He might have spoken, but I did not hear; one hand was stretched out, seemingly to detain me, but I escaped and rushed downstairs...\n",
        "```\n",
        "\n",
        "**Assessment:**\n",
        "- **Style match**: 4/5 - Captures Gothic, archaic tone well\n",
        "- **Coherence**: 3/5 - Makes sense in short spans (~50 chars), breaks down over longer spans\n",
        "- **Repetition**: Yes - With greedy sampling, model gets stuck in loops after ~200 chars\n",
        "- **Temperature effect**: T=0.8 significantly reduces repetition while maintaining style\n",
        "\n",
        "### Accuracy Evaluation\n",
        "\n",
        "**Letter 1 Model on Letter 1 Prompts:**\n",
        "- Test 1: 11.11% accuracy\n",
        "- Test 2: 6.02% accuracy  \n",
        "- Test 3: 31.07% accuracy\n",
        "- **Average: 16.06%** ← Decent for sampling!\n",
        "\n",
        "**Full Novel Model on Letter 1 Prompts:**\n",
        "- Test 1: 3.51% accuracy\n",
        "- Test 2: 8.27% accuracy\n",
        "- Test 3: 10.68% accuracy\n",
        "- **Average: 7.49%** ← Lower but model is better overall!\n",
        "\n",
        "**Key Insight**: Character accuracy is misleading with temperature sampling. Both models generate coherent Frankenstein-style text, but the full novel model has broader knowledge.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🔬 Experiments Conducted\n",
        "\n",
        "### Experiment 1: Temperature Sampling ✅\n",
        "\n",
        "**Hypothesis:**  \n",
        "Temperature sampling (T=0.8) will reduce repetition while maintaining coherence.\n",
        "\n",
        "**Setup:**  \n",
        "- Changed from `torch.argmax(last_logits)` to temperature sampling\n",
        "- Formula: `probs = torch.softmax(last_logits / 0.8, dim=-1)`\n",
        "- Sample: `next_id = torch.multinomial(probs, num_samples=1).item()`\n",
        "\n",
        "**Result:**  \n",
        "- Generated 2,000 characters with much less repetition\n",
        "- Style maintained (Gothic, archaic)\n",
        "- More diverse vocabulary usage\n",
        "- Still coherent over short spans\n",
        "\n",
        "**Conclusion:**  \n",
        "Temperature sampling essential for text generation. T=0.8 optimal balance for creativity/coherence.\n",
        "\n",
        "### Experiment 2: Full Novel Training ✅\n",
        "\n",
        "**Hypothesis:**  \n",
        "Training on entire novel will improve model quality and generation diversity.\n",
        "\n",
        "**Setup:**  \n",
        "- Trained on 442K characters (vs 6,850)\n",
        "- Vocab size increased from 60 to 93\n",
        "- Learning rate reduced from 0.015 to 0.003\n",
        "- Trained for 10 epochs\n",
        "\n",
        "**Result:**  \n",
        "- Loss converged beautifully: 1.52 → 1.29\n",
        "- Character accuracy lower on Letter 1 prompts (7.49% vs 16.06%)\n",
        "- BUT generates more diverse, less repetitive text\n",
        "- Model learned broader patterns from entire novel\n",
        "\n",
        "**Conclusion:**  \n",
        "More data doesn't always mean higher accuracy on specific prompts. Model generalized better overall.\n",
        "\n",
        "### Experiment 3: Learning Rate Adjustment ✅\n",
        "\n",
        "**Hypothesis:**  \n",
        "Learning rate 0.015 too high for large dataset → training will diverge.\n",
        "\n",
        "**Setup:**  \n",
        "Initial training with lr=0.015 showed:\n",
        "- Epoch 1: 1.5117\n",
        "- Epoch 2: 1.5146\n",
        "- Epoch 3: 1.6102 ← Increasing!\n",
        "- Epoch 4: 1.6288 ← Diverging!\n",
        "\n",
        "**Result:**  \n",
        "Training diverged, loss increased. Fixed by reducing to lr=0.003, training converged properly.\n",
        "\n",
        "**Conclusion:**  \n",
        "Learning rate MUST scale with dataset size. Rule of thumb: larger dataset = smaller learning rate.\n",
        "\n",
        "### Experiments to Try Next\n",
        "\n",
        "- [ ] **Validation Split**: Implement 80/20 train/val split to monitor overfitting\n",
        "- [ ] **Longer Training**: 20-30 epochs to see if accuracy improves\n",
        "- [ ] **Beam Search**: Implement top-k sampling for better generation\n",
        "- [ ] **Different Temperature Values**: Compare T=0.5, 0.8, 1.0, 1.2\n",
        "- [ ] **Larger Model**: Increase hidden_size from 96 to 128 or 256\n",
        "- [ ] **Word-Level Model**: Compare word vs character tokenization\n",
        "- [ ] **Attention Mechanism**: Add attention for better long-range dependencies\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 💡 Insights & Breakthroughs\n",
        "\n",
        "### What Surprised Me\n",
        "\n",
        "1. **Character accuracy is misleading**: 16% accuracy sounds bad, but the text reads well! Temperature sampling introduces randomness, so exact character matching isn't the right metric.\n",
        "\n",
        "2. **Lower learning rate needed for larger datasets**: Initially thought more data = faster training with same LR. Wrong! More batches per epoch require smaller steps.\n",
        "\n",
        "3. **Full novel model \"worse\" on Letter 1**: Trained on entire novel, tested on Letter 1 → lower accuracy (7.49% vs 16.06%). But this doesn't mean it's worse! Model learned broader patterns, works better on diverse prompts.\n",
        "\n",
        "4. **Temperature sampling is essential**: Greedy sampling (argmax) creates repetitive loops. Even small temperature (T=0.8) adds creativity without breaking coherence.\n",
        "\n",
        "5. **Sliding windows elegantly create training data**: Labels = features shifted by 1 position. Such a simple way to generate (input, target) pairs!\n",
        "\n",
        "### Biggest Challenge\n",
        "\n",
        "**Understanding train/test mismatch and evaluation metrics.**\n",
        "\n",
        "Initially thought: \"Lower accuracy = worse model.\"  \n",
        "Reality: Accuracy depends on train/test distribution match.\n",
        "\n",
        "Letter 1 model trained on Letter 1 → tested on Letter 1 → high accuracy  \n",
        "Full novel model trained on everything → tested on Letter 1 → lower accuracy\n",
        "\n",
        "**Solution**: Evaluate on appropriate test set. Full novel model should be tested on diverse prompts from entire novel.\n",
        "\n",
        "### Most Valuable Learning\n",
        "\n",
        "**Three critical insights:**\n",
        "\n",
        "1. **Learning rate scales with dataset size** - Don't use same LR for different dataset sizes\n",
        "2. **Evaluation must match training distribution** - Test on what you trained on\n",
        "3. **Quality > Accuracy** - Generated text quality matters more than character-level metrics\n",
        "\n",
        "These principles apply to ANY sequence modeling task!\n",
        "\n",
        "### Connection to Other Projects\n",
        "\n",
        "**Transferable Concepts:**\n",
        "\n",
        "- **LSTM gates**: Similar to attention mechanisms (what to focus on)\n",
        "- **Hidden state carryover**: Like RNNs in general, but with better memory\n",
        "- **Temperature sampling**: Used in GPT, BERT, all modern LLMs\n",
        "- **Sliding windows**: Like CNNs but for sequences\n",
        "- **CrossEntropyLoss**: Same loss used in classification tasks\n",
        "- **Autoregressive generation**: Foundation of GPT models\n",
        "\n",
        "**Next Steps for Periospot AI:**\n",
        "- Apply LSTMs to dental text classification\n",
        "- Use character-level models for clinical note generation\n",
        "- Implement temperature sampling for diverse AI responses\n",
        "- Scale learning rates appropriately for healthcare datasets\n",
        "\n",
        "### Breakthrough Moment\n",
        "\n",
        "When I reduced learning rate from 0.015 to 0.003 and saw training actually converge (loss decreasing smoothly). This taught me that hyperparameter tuning isn't guesswork - there are principles behind it!\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🚀 Next Steps\n",
        "\n",
        "### Short-Term (This Week)\n",
        "1. ✅ Complete notebooks 00-06 - DONE!\n",
        "2. ✅ Train full novel model - DONE!\n",
        "3. ✅ Implement temperature sampling - DONE!\n",
        "4. ✅ Test accuracy evaluation - DONE!\n",
        "5. Write comprehensive lab notes (this notebook!)\n",
        "\n",
        "### Medium-Term (This Month)\n",
        "1. Implement validation split (80/20) to track overfitting\n",
        "2. Train model for 20-30 epochs to see improvement\n",
        "3. Test full novel model on diverse prompts from different chapters\n",
        "4. Implement beam search or top-k sampling\n",
        "5. Compare greedy vs temperature vs beam search\n",
        "\n",
        "### Long-Term (This Quarter)\n",
        "1. Build word-level LSTM model (faster training, better for classification)\n",
        "2. Implement attention mechanism for long-range dependencies\n",
        "3. Apply to Periospot AI clinical text tasks\n",
        "4. Experiment with Transformer architecture (BERT, GPT-style)\n",
        "5. Compare character vs word vs subword tokenization\n",
        "\n",
        "### Integration Goals\n",
        "- Use LSTM knowledge for dental text classification\n",
        "- Apply to clinical note generation with temperature sampling\n",
        "- Implement proper train/val/test splits for medical datasets\n",
        "- Scale learning rates appropriately for healthcare data size\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📚 Resources & References\n",
        "\n",
        "### Helpful Links\n",
        "- [PyTorch LSTM Documentation](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)\n",
        "- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "- [The Unreasonable Effectiveness of RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        "\n",
        "### Similar Projects to Explore\n",
        "- Word-level language models\n",
        "- Text classification with LSTM\n",
        "- Sequence-to-sequence models\n",
        "- Transformer-based text generation (GPT-style)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ✨ Final Reflection\n",
        "\n",
        "### What This Project Taught Me\n",
        "\n",
        "**Technical Skills:**\n",
        "- Character-level tokenization and vocabulary building\n",
        "- PyTorch Dataset and DataLoader implementation\n",
        "- LSTM architecture (Embedding → LSTM → Linear)\n",
        "- Training loop mechanics (zero_grad, forward, loss, backward, step)\n",
        "- Autoregressive text generation\n",
        "- Temperature sampling for controlled randomness\n",
        "- Model saving and loading\n",
        "- Learning rate tuning based on dataset size\n",
        "\n",
        "**ML Principles:**\n",
        "- Train/test distribution matching is critical\n",
        "- Learning rate must scale with dataset size\n",
        "- Quality metrics matter more than raw accuracy\n",
        "- Hyperparameter tuning has systematic principles\n",
        "- More data doesn't always mean better specific task performance\n",
        "- Evaluation methodology matters\n",
        "\n",
        "**System Design:**\n",
        "- Proper data splitting (train/val/test)\n",
        "- Monitoring training (loss curves, convergence)\n",
        "- Version control for models (save/load weights)\n",
        "- Reproducibility (consistent random seeds, saved configs)\n",
        "\n",
        "### How I Grew as a Machine Learning Practitioner\n",
        "\n",
        "**Before this project:** I knew about LSTM theory but hadn't implemented one from scratch.\n",
        "\n",
        "**After this project:** I can:\n",
        "- Design and implement LSTM architectures\n",
        "- Debug training issues (divergence, overfitting)\n",
        "- Tune hyperparameters systematically\n",
        "- Evaluate models appropriately\n",
        "- Generate text with sampling techniques\n",
        "\n",
        "**Key Growth Area:** Understanding that ML isn't just math - it's practical engineering. Hyperparameter choices matter, evaluation strategy matters, and intuition comes from experience.\n",
        "\n",
        "### What I Would Do Differently Next Time\n",
        "\n",
        "1. **Start with validation split**: Would implement train/val/test from the beginning to catch overfitting early\n",
        "2. **Use wandb or tensorboard**: Better visualization of training progress\n",
        "3. **Experiment systematically**: Keep a proper experiment log with all hyperparameters\n",
        "4. **Test on diverse prompts**: Would create test set from entire novel, not just Letter 1\n",
        "5. **Try word-level first**: Character-level is computationally expensive - word-level might be faster to iterate\n",
        "\n",
        "### My Favorite Part of This Project\n",
        "\n",
        "**The moment when the generated text actually looked like Frankenstein!**\n",
        "\n",
        "When I first saw: \"You will rejoice to hear that no disaster has accompanied the commencement of an enterprise which have been made in the prospect of arriving at the pole...\"\n",
        "\n",
        "I was amazed that:\n",
        "1. The model learned actual words (not gibberish)\n",
        "2. It captured the Gothic, archaic style\n",
        "3. Punctuation and capitalization were correct\n",
        "4. Sentence structure made sense\n",
        "\n",
        "This showed me that neural networks really CAN learn complex patterns from data. The LSTM's ability to remember context across characters created coherent, styled text.\n",
        "\n",
        "**Special Moment:** When I reduced the learning rate and saw training converge smoothly. That's when I truly understood that ML hyperparameter tuning isn't random - there are principles!\n",
        "\n",
        "### Project Summary\n",
        "\n",
        "**Models Trained:**\n",
        "- Letter 1 model: 64,764 parameters, vocab_size=60, loss→1.7\n",
        "- Full novel model: 69,549 parameters, vocab_size=93, loss→1.29\n",
        "\n",
        "**Key Achievements:**\n",
        "- ✅ Complete character-level language model from scratch\n",
        "- ✅ Successful text generation in Mary Shelley's style\n",
        "- ✅ Understanding of LSTM gates, states, and training\n",
        "- ✅ Experience with temperature sampling\n",
        "- ✅ Learning rate tuning for different dataset sizes\n",
        "- ✅ Proper evaluation and accuracy metrics\n",
        "\n",
        "**Final Takeaway:**\n",
        "This is honest work. Building from scratch taught me more than following tutorials. The mistakes (diverging training, vocab mismatches) were the best teachers. Now I understand WHY things work, not just HOW to make them work.\n",
        "\n",
        "**Next Chapter:**\n",
        "Apply this knowledge to Periospot AI - dental text classification, clinical note generation, and healthcare NLP!\n",
        "\n",
        "---\n",
        "\n",
        "*Keep learning, keep building, keep reflecting.* 🚀\n",
        "\n",
        "**Project Completion Date:** 2024  \n",
        "**Total Time Invested:** ~8-10 hours  \n",
        "**Notebooks Completed:** 7/7 (00-06, 99)  \n",
        "**Models Trained:** 2  \n",
        "**Lines of Generated Text:** 2,500+ characters  \n",
        "**Knowledge Gained:** Invaluable!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
